\documentclass[a4paper,10pt]{article}
\include{header}
\title{Lecture 16: Martingales}
\author{Parimal Parag}
\begin{document}
\maketitle
\section{Martingales}
A martingale is a type of stochastic process whose definition formalizes the concept of a fair game.
\begin{defn}
A stochastic process $\{Z_n,~n \geq 1\}$ is said to be a martingale process if 
\begin{enumerate}
\item $E[|Z_n|]< \infty$, ~ \text{for all}~ n.
\item $E[Z_{n+1}|Z_1,Z_2, \hdots Z_n]=Z_n$.
\end{enumerate}
\end{defn}
Taking expectation on both sides of part 2 of the above definition, we get
\begin{equation*}
E[Z_{n+1}]=E[Z_n],
\end{equation*}
and so 
\begin{equation*}
E[Z_{n+1}]=E[Z_1],~ \text{for all} ~ n.
\end{equation*}
\begin{exmp}
Let $\{X_i\}$ be a sequence of independent random variables with mean $0$. Let $Z_n=\sum_{i=1}^n X_i$. Then, $\{Z_n,~n \geq 1\}$ is a martingale. This is so because, $E[Z_n]=0$ and 
\begin{flalign*}
E[Z_{n+1}|Z_1,Z_2 \hdots Z_n]&=E[Z_{n}+X_{n+1}|Z_1,Z_2 \hdots Z_n]\\
&=E[Z_{n}|Z_1,Z_2 \hdots Z_n]+E[X_{n+1}|Z_1,Z_2 \hdots Z_n]\\
&=Z_n.
\end{flalign*} 
\end{exmp}
\begin{exmp}
Let $\{X_i\}$ be a sequence of independent random variables with mean $1$. Let $Z_n=\Pi_{i=1}^n X_i$. Then, $\{Z_n,~n \geq 1\}$ is a martingale. This is so because, $E[Z_n]=1$ and 
\begin{flalign*}
E[Z_{n+1}|Z_1,Z_2 \hdots Z_n]&=E[Z_{n}X_{n+1}|Z_1,Z_2 \hdots Z_n]\\
&=Z_nE[X_{n+1}|Z_1,Z_2 \hdots Z_n]\\
&=Z_nE[X_{n+1}]\\
&=Z_n.
\end{flalign*} 
\end{exmp}
\begin{exmp}
Let $\{X_n\}$ be a branching process. Let $X_0=1$. Then,
\begin{equation*}
X_n = \sum_{i=1}^{X_{n-1}}Z_i,
\end{equation*}
where $Z_i$ represents the number of offspring of the $i^{\text{th}}$ individual of the $(n-1)^{\text{st}}$ generation. conditioning on $X_{n-1}$ yields, $E[X_n]= \mu^n$ where $\mu$ is the mean number of offspring per individual. Then $Y_n$ is a martingale when 
\begin{equation*}
Y_n =X_n / \mu^n.
\end{equation*}
This is true because $E[Y_n]= 1$ and 
\begin{flalign*}
E[Y_{n+1}|Y_1, \hdots Y_n]&= \frac{1}{\mu^{n+1}}E[X_{n+1}|Y_1, \hdots Y_n]\\
&= \frac{1}{\mu^{n+1}}E[\sum_{i=1}^{X_{n}}Z_i|Y_1, \hdots Y_n]\\
&=  \frac{1}{\mu^{n+1}}X_{n}E[Z_i]= \frac{X_n}{\mu^n}=Y_n.\\
\end{flalign*}
\end{exmp}
\begin{exmp}
Let $X,Y_1,Y_2 \hdots$ be arbitrary random variables such that $E[|X|]< \infty$. Then
\begin{equation*}
Z_n =E[X|Y_1,Y_2, \hdots Y_n]
\end{equation*}
is a martingale. The integrability condition is direct to be verified.
\begin{flalign*}
E[Z_{n+1}|Y_1,Y_2, \hdots Y_n]&= E[E[X|Y_1,\hdots Y_{n+1}]|Y_1,\hdots Y_{n}]\\
&= E[X|Y_1,\hdots Y_{n}]]=Z_n.\\
\end{flalign*} 
Thus the result follows. The above martingale is called the Doob type martingale.
\end{exmp}
\begin{exmp}
For any sequence of random variables $X_1,X_2 \hdots $, the random variables $X_i-E[X_i|X_1 \hdots X_{i-1}]$ have zero mean. Define
\begin{equation*}
Z_n =\sum_{i=1}^n X_i -E[X_i|X_1,X_2, \hdots X_{i-1}] 
\end{equation*}
 is  a martingale provided $E[|Z_n|]< \infty$.  To verify the same, 
 \begin{flalign*}
E[Z_{n+1}|Z_1 \hdots Z_n]&= E[Z_n+X_n-E[X_n|X_1 \hdots X_{n-1}]]\\
&= Z_n+E[X_n-E[X_n|X_1 \hdots X_{n-1}]]=Z_n.\\
\end{flalign*}
\end{exmp}
\subsection{Stopping Times}
\begin{defn}The positive integer values, possibly infinite, random variable $N$ is said to be a random time for the process $\{Z_n\}$ if the event $\{N=n\}$ is determined by the random variables $Z_1 \hdots Z_n$. If $Pr(N < \infty)=1$, then the random time $N$ is said to be  a stopping time.  
\end{defn}
\begin{defn}
Let $N$ be a random time for the process $\{Z_n\}$. Let
\begin{eqnarray*}
\bar{Z}_n = \left\{
     \begin{array}{lr}
       Z_n & : n \leq N\\
      Z_N & : n > N.
     \end{array}
   \right.
\end{eqnarray*}
$\{\bar{Z}_n\}$ is called the stopped process.
\end{defn}
\begin{prop}
If $N$ is a random time for the martingale $\{Z_n\}$, then the stopped process $\{\bar{Z}_n\}$ is also a martingale.
\end{prop}
\begin{proof}
We claim that 
\begin{equation*}
\bar{Z}_n= \bar{Z}_{n-1}+1_{N \geq n}(Z_n-Z_{n-1})
\end{equation*}
The above equation can be directly verified by considering the two cases separately viz. 
\begin{enumerate}
\item $N \geq n$: $\bar{Z}_n=Z_n$.
\item $N < n:$ $\bar{Z}_{n-1}=\bar{Z}_{n}=Z_N$
\end{enumerate}
\begin{flalign*}
E[\bar{Z}_{n+1}|Z_1 \hdots \bar{Z}_n]&=E[\bar{Z}_{n}+1_{n \leq N}(Z_n-Z_{n-1})|Z_1 \hdots \bar{Z}_n]\\
&\stackrel{(a)}{=}\bar{Z}_{n}+1_{n \leq N} E[(Z_n-Z_{n-1})|Z_1 \hdots \bar{Z}_n]\\
&=\bar{Z}_{n},
\end{flalign*}
\end{proof}
where in $(a)$ we have used the fact that $N$ is a random time. Also, we have $E[\bar{Z}_{n}]=E[Z_1]$, for all $n$.  Now assume that $N$ is a stopping time. It is immediate 
\begin{equation*}
that \bar{Z}_n \rightarrow Z_N ~ \text{w.p}~ 1.
\end{equation*}
But  is it true that
\begin{equation*}
that E[\bar{Z}_n] \rightarrow E[Z_N] ~ \text{as n}~ \rightarrow \infty.
\end{equation*}
It so turns out that the above is true under some additional regularity constraints only. We state the following theorem without proof.
\begin{thm}
If either:
\begin{enumerate}
\item $\bar{Z}_n$ are uniformly bounded, or;
\item $N$ is bounded, or;
\item $E[N]< \infty$, and there is an $M< \infty$ such that
\begin{equation*}
E[|Z_{n+1}-Z_n||Z_1 \hdots Z_n]< M,
\end{equation*}
then $E[\bar{Z}_n]\rightarrow E[Z_N]=E[Z_1]$.
\end{enumerate}
\end{thm}
\begin{cor}
\textbf{Wald's Equation}: If $X_i,~ i \geq 1$, are independent and identically distributed \textit{iid} with $E[|X|]< \infty$ and if $N$ is a stopping time for $X_1,X_2 \hdots $ with $E[N]< \infty$, then
\begin{equation*}
E[\sum_{i=1}^{N}X_i]=E[N]E[X].
\end{equation*}
\end{cor}
\begin{proof}
Let $\mu=E[X]$. Since 
\begin{equation*}
Z_n = \sum_{i=1}^{n}(X_i-\mu)
\end{equation*}
is a martingale and hence from the previous theorem, 
\begin{equation*}
E[Z_N]=E[Z_1]=0.
\end{equation*}
But 
\begin{flalign*}
E[Z_N]&= E[\sum_{i=1}^N (X_i-\mu)]\\
&=E[\sum_{i=1}^N (X_i)-N\mu)]\\
&=E[\sum_{i=1}^N (X_i)]-E[N]\mu)].
\end{flalign*}
Observe that condition $3$  for Martingale stopping theorem to hold can be directly verified. Hence the result follows. 
\end{proof}
\end{document}