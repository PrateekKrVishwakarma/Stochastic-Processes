  \documentclass[a4paper,10pt]{article}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage[hidelinks]{hyperref}

\newtheorem{prop}{Proposition}
\newtheorem{defi}{Definition}
\newtheorem{theo}{Theorem}
\newtheorem{lem}{Lemma}

\title{SPQT Lecture 3: Limit theorems in Renewal Theory}
\author{Prof. Parimal Parag}

\begin{document}
\maketitle
\section{Renewal Theory Contd - Limit Theorems}
\subsection{Basic Renewal Theorem}
Let $N(\infty) := \lim_{t \to \infty} N(t)$. Then $P[N(\infty) = \infty] = 1$.
\begin{proof}
It suffices to show $P[N(\infty) < \infty] = 0$. We have
\begin{flalign}
P[N(\infty) < \infty] &= P[\cup_{n \in \mathbb{N}} \{N(\infty) < n\}] \\
&= P[\cup_{n \in \mathbb{N}} \{S_n = \infty\}] \\
&= P[\cup_{n \in \mathbb{N}} \{X_n = \infty\}] \\
&\leq \sum_{n \in \mathbb{N}}P[X_n = \infty]\\
&=0
\end{flalign}
The last step follows from the fact that $E[X_n] < \infty$.
\end{proof}

\begin{prop}
\[\lim_{t \to \infty} \frac{N(t)}{t} = \frac{1}{\mu} \quad \mbox{a.s.}\]
\end{prop}
\begin{proof}
Consider $S_{N(t)}$. By definition, we have
\[S_{N(t)} \leq t < S_{N(t)+1}\]
Dividing by $N(t)$, we get 
\[\frac{S_{N(t)}}{N(t)} \leq \frac{t}{N(t)} < \frac{S_{N(t)+1}}{N(t)}\]
By Strong Law of Large Numbers (SLLN) and the previous result, we have
\[\lim_{t \to \infty}\frac{S_{N(t)}}{N(t)} = \mu \quad \mbox{a.s.}\] 
Also
\[\lim_{t \to \infty} \frac{S_{N(t)+1}}{N(t)} = \lim_{t \to \infty} \frac{S_{N(t)+1}}{N(t)+1}.\frac{N(t)+1}{N(t)} \]
Hence by Squeeze Theorem, the result follows.
\end{proof}
\subsubsection{Example}
Consider a simple coin tossing game wherein a head is counted as a win whereas tails is counted as a loss. Let $N(n)$ denote the number of losses when the coin is tosses $n$ times. Let us assume that the probability of winning in any game $X$ is uniformly distributed $(0,1)$. Then 
\[P_W(n) = \frac{n-N(n)}{n}\]
is the fraction of wins. We shall show that this fraction approaches 1 as $n \to \infty$. Hence by the previous proposition, and noting that time till first loss is geometrically distributed, we have:
\begin{flalign}
\lim_{n \to \infty} \frac{N(n)}{n} &= \frac{1}{E[\mbox{Time till first loss}]} \\
&= \frac{1}{E\left[\frac{1}{1-X}\right]} = \frac{1}{\infty} = 0
\end{flalign}
Hence Renewal theorems can be used to compute these long term averages. More theorems to come...

\subsection{Wald's Lemma}
Before we get into Wald's Lemma, let us first define what a stopping time is.
\begin{defi}[Stopping Time]
Let $\{X_n: n\in \mathbb{N}\}$ be independent random variables. Then $T$, an integer random variable, is called a stopping time wrt this sequence if $\{N=n\}$ depends only on $\{X_1,\cdots,X_n\}$ and is independent of $X_{n+1}, X_{n+2},\cdots$. 
\end{defi}
The intuition behind a stopping time is that it's value is determined by past and present events but NOT by future events. For instance, while traveling on the bus, the random variable measuring ``Time until bus crosses Majestic and after that one stop" is a stopping time as it's value is determined by events before it happens. On the other hand ``Time until bus stops before Majestic is reached'' would not be a stopping time  in the same context. This is because we have to cross this time, reach Majestic and then realise we have crossed that point. Another example is consider $X_n$ iid Bernoulli$(1/2)$. Then $N = min \{n \in \mathbb{N}:\quad \sum_{k=1}^n X_i = 10\}$ is a stopping time.


\textbf{Exercise}: Try to list out properties of stopping times. For instance, sum of two stopping times is a stopping time. Minimum of two stopping times is a stopping time. See how many can you find and prove/disprove.

\begin{lem}[Wald's Lemma]
Let $\{X_i, \quad i\in \mathbb{N}\}$ be iid random variables with finite mean $E[X_1]$ and let $N$ be a stopping time wrt these set of variables. Assume $E[N] < \infty$. Then
\[E\left[\sum_{n=1}^N X_n\right] = E[X_1]E[N]\]
\end{lem}
\begin{proof}
\begin{flalign}
E\left[\sum_{n=1}^N X_n\right] &= E\left[\sum_{n \in \mathbb{N}} X_n 1_{\{N \geq n\}}\right]  \\
&= \sum_{n \in \mathbb{N}} E\left[X_n 1_{\{N \geq n\}}\right] \label{tricky}
\end{flalign}
I'd like to point out here that in step (\ref{tricky}), you cannot always exchange infinite sums and expectations. But here you can do so. Refer Ross/Wolff if you are interested. Regardless, to proceed, we need to show that $N \geq n$ is independent of $X_k, \quad k \geq n$. This is fixed by observing that $\{N \geq k\} = \{N < k\}^c = \{N \leq k-1\}^c = [\cup_{i=1}^{k-1} \{N = i\}]^c$. What I'm getting at is that we can deduce whether $N \geq k$ by looking at the first $k-1$ samples and is independent of the future and present samples.

Hence, we get
\begin{flalign}
\sum_{n \in \mathbb{N}} E\left[X_n 1_{\{N \geq n\}}\right] &= \sum_{n \in \mathbb{N}} E\left[X_n\right]E\left[ 1_{\{N \geq n\}}\right] \\
&= E\left[X_1\right] \sum_{n \in \mathbb{N}} P[N \geq n] = E[X_1]E[N]
\end{flalign} 
\end{proof}

\begin{prop}
$N(t)+1$ is a stopping time where $X_1,X_2,\cdots$ iid are interarrival times of a renewal process $N(t)$ with $E[X_1] < \infty$
\end{prop}
\begin{proof}
\[
\left\{N(t) + 1 = n \right\} \iff \{S_{n-1} \leq t < S_n\} \iff \{\sum_{i=1}^{n-1} X_i \leq t < (\sum_{i=1}^{n-1} X_i) + X_n\}  \]
Thus $N(t)+1$ is a stopping time. Hence from Wald's Lemma
\[ E\left[\sum_{i=1}^{N(t)+1}X_i\right] = E[X_1]E[1 + N(t)] = E[X_1][1+m(t)]\]
\end{proof}

\subsection{Elementary Renewal Theorem}
Essentially Elementary Renewal theorem says that under the hypotheses of Basic renewal theorem, we have (recall $m(t) = E[N(t)]$)
\begin{theo}
\[\lim_{t \to \infty}\frac{m(t)}{t} = \frac{1}{\mu}\]
\end{theo}
Before we proceed, we remark that it does \textbf{not} obviously follow from basic renewal theorem by taking Expected value on both sides. Consider the following example
\[
Y_n = \left\{ \begin{array}{l l}
n^2 & \mbox{w.p.  } 1/n^2 \\
0 & \mbox{w.p.  } 1- 1/n^2
\end{array} \right.
\]
Then for any $\epsilon > 0$, $P[Y_n > \epsilon] = P[Y_n = n^2] = 1/n^2$. Then $\sum P[Y_n > \epsilon] < \infty$. Hence by Borel Cantelli Lemma, $Y_n \to 0$ a.s. However, $E[Y_n] = 1$ for all $n \in \mathbb{N}$. So $E[Y_n] \to 1$. QED.
\begin{proof}
Take $\mu < \infty$. We have $S_{N(t)+1} > t$. Hence we have
\begin{flalign}
S_{N(t)+1} > t &\Rightarrow  E[S_{N(t)+1}] > t \\
&\Rightarrow \mu [1+m(t)] > t \\
&\Rightarrow \frac{m(t)}{t} > \frac{1}{\mu} - \frac{1}{t} \\
&\Rightarrow \liminf_{t \to \infty} \frac{m(t)}{t} \geq \frac{1}{\mu}  
\end{flalign}

Thus we now have to show 
\[\limsup_{t \to \infty} \frac{m(t)}{t} \leq \frac{1}{\mu}\]
We shall argue this using a truncated random variables argument. Define $\bar{X}_n = X_n 1_{\{X_n \leq M\}} + M1_{\{X_n > M\}}$. Analogously, we define $\bar{S}_n = \sum_{k=1}^n \bar{X}_k$ and $\bar{N}(t) = \sup\{n \in \mathbb{N}: \bar{S}_n \leq t\}$. Thus we have
\[t < \bar{S}_{N(t)+1} \leq t+M\]
since the next renewal can only happen in at most $M$ units of time. Once again, we have
\[E[\bar{S}_{N(t)+1}] = \mu_M E[1+\bar{N}(t)] \leq t+M\]
\[E\left[\frac{\bar{N}(t)}{t}\right] \leq \frac{1}{\mu_M} + \frac{M-\mu_M}{t\mu_M}\]
Since $\bar{X}_i \leq X_i$, we have $N(t) \leq \bar{N}(t)$ (try to understand this). Hence
\[E\left[\frac{{N}(t)}{t}\right] \leq \frac{1}{\mu_M} + \frac{M-\mu_M}{t\mu_M}\]
Now take $\limsup$ on both sides to get
\[\limsup_{t \to \infty}E\left[\frac{{N}(t)}{t}\right] \leq \frac{1}{\mu_M}\]
Now observe that LHS is independent of $M$. Take limits $M \to \infty$, noting that $\mu_M \to \mu$ (Why?) to get
 \[\limsup_{t \to \infty}\frac{m(t)}{t} \leq \frac{1}{\mu}\]
Putting it all together,
\[\lim_{t \to \infty}\frac{m(t)}{t} = \frac{1}{\mu}\]
\end{proof}

\subsection{Central Limit for Renewal Processes}
\begin{theo}
Let $X_n$ be iid random variables with $\mu = E[X_n] < \infty$ and $\sigma^2 = Var(X_n) < \infty$. Then
\[\frac{N(t)-\frac{t}{\mu}}{\sigma \sqrt{\frac{t}{\mu^3}}} \to^d N(0,1) \]
\end{theo}
\begin{proof}
Take $u = \frac{t}{\mu} + y \sigma \sqrt{\frac{t}{\mu^3}}$. We shall treat $u$ as an integer and proceed, the proof for general $u$ is an exercise. 
Thus 
\begin{flalign}
P[N(t) < u] &= P[S_u >t] \\
&= P\left[\frac{S_u - u\mu}{\sigma \sqrt{u}} > \frac{t - u\mu}{\sigma \sqrt{u}}\right]
\end{flalign}

\end{proof}
\end{document}