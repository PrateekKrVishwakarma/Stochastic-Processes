% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\include{header}
\title{Lecture 12 : Convergence of DTMCs and Coupling theorem}
\author{}
\begin{document}
\maketitle
\section{Discrete Time Markov Chains Contd.}

\subsection{Total Variation Distance}

Given two probability distributions p and q defined on set of natural numbers \textbf{$N_0$}, their total variation distance is defined as\\
$d_{TV}(p,q) = \dfrac{1}{2} \lVert p - q \rVert_1$\\
\textbf{FACT:} $d_{TV}(p,q) = sup_{S \subseteq N_0} [p(S) - q(S)]$
\textbf{Definition: Convergence in total variation\\}\\
Let ${X_n}_{n \geq 0}$ be a $N_0$-valued stochastic process if $\exists$ a probability distribution $\Pi$ on $N_0$ such that \\
$lim_{n \rightarrow \infty} d_{TV}(P[X_{n} \in .], \Pi) = 0$ i.e.\\

$lim_{n \rightarrow \infty} \sum{i \in N_{0}} \lvert P[X_{n}=i] - \Pi(i) \rvert = 0$\\
Then, we say that $P[X_{n} \in .] \rightarrow \Pi$ in total variation distance as $n \rightarrow \infty$.\\
\textbf{NOTE:}\\
If $X_{n} \rightarrow \Pi$, then $\forall$ bounded functions, $f:N_{0} \rightarrow R$, $lim_{n \rightarrow \infty} E[f(X{n})] = \Sigma_{i \in N_0} \Pi(i)f(i)$\\
\textbf{Theorem: Convergence in variation of DTMC}\\
Let ${X_n}_{n \geq 0}$ be an ergodic DTMC on $N_{0}$ (irreducible, aperiodic and positive recurrent) with stationary distribution $(\Pi_{j})_{j \in N_0}$.\\
Then $\forall$ initial distribution of $X_0$, we have $X_n \rightarrow \Pi$.\\
In other words, if $\mu_j$ is the initial distribution, then \\

 $lim_{n \rightarrow \infty} \lVert \mu P^n - \Pi \rVert_1 = 0$\\
 where, $P_{ij} = P[X_{n+1} = j/X_n = i]$\\

 
\subsection{The Coupling method}
\textbf{Definition:}\\
Consider two stochastic processes $\{X_n'\}_n$ and $\{X_n''\}_n$ on $N_0$, defined on the same probability space.\\
$\{X_n'\}_n$ and $\{X_n''\}_n$ are said to couple if $\exists$ an a.s. finite random time $\tau$ such that $\forall n \geq \tau : X_n' = X_n''$ a.s.\\
Moreover, $\tau$ is called a coupling time of the process.\\

\textbf{Coupling Inequality:}\\
If $\tau$ is a coupling time, then $\forall n \geq 0$,
$d_{TV} (X_n',X_n'') = \Sigma_{i \in N_0} \lvert P[X_n' = i] - P[X_n'' = i] \rvert \leq P[\tau > n]$\\

Remark: Variation distance is bounded based on the coupling time.\\

\textbf{Proof:} Consider any $A \subseteq N_0$.\\
\begin{eqnarray*}
\begin{aligned}
& P[X_n' \in A] - P[X_n'' \in A] = P[X_n' \in A,\tau \leq n] + P[X_n' \in A,\tau > n] - P[X_n'' \in A, \tau \leq n] - P[X_n'' \in A,\tau > n] \\
& P[X_n' \in A] - P[X_n'' \in A] = P[X_n' \in A,\tau > n] - P[X_n'' \in A,\tau > n] (from \: the \: definition \: of \: coupling \: between \: X_n' \: and \: X_n'')\\
& P[X_n' \in A] - P[X_n'' \in A] \leq P[X_n' \in A,\tau > n] \leq P[\tau > n]\\
\end{aligned}
\end{eqnarray*}
 

\subsection{Theorem[DTMC Convergence in $d_{TV}$]}	
Let ${X_n}$ be a homogenous $N_0$ valued ergodic DTMC with transition probability $p_{ij}$ with stationary distribution $(\Pi_j)_{j \in N_0}$. Then for any initial distribution of $X_0$,\\
\begin{eqnarray*}
	\begin{aligned}
 lim_{n \longrightarrow \infty} \sum_{i \in N_0} \lvert P[X_n = i] - \Pi(i) \rvert = 0\\
\end{aligned}
\end{eqnarray*}
\textbf{Proof:}
We will prove using the coupling argument.\\
Let $\{X_n^{(1)}\}_{n \geq 0}$ and $\{X_n^{(2)}\}_{n \geq 0}$ be two independent ergodic DTMCs with transition matrix P and initial distributions $\mu$ and $\Pi$ respectively.\\
Construct the product DTMC $Z_n = (X_n^{(1)}, X_n^{(2)})_{n \in N_0}$\\
$\{Z_n\}_{n \geq 0}$ has transition probabilities\\
\begin{eqnarray*}
\begin{aligned}
	P[Z_n = (i,j)/Z_{n-1} = (k,l)] = p_{ki} p_{lj}
\end{aligned}
\end{eqnarray*}
\textbf{CLAIMS:}
\begin{itemize}
	\item $\{Z_n\}_{n \geq 0}$ is irreducible and aperiodic.
	\item $\{Z_n\}_{n \geq 0}$ is positive recurrent.
\end{itemize}
\textbf{Proof:}\\
$\Pi_z(i,j) = \Pi(i) \Pi(j)$ is a stationary distribution
$$
\tau = 
\begin{cases}
inf  \{n \geq 0: X_n^{(1)} = X_n^{(2)}\}\\
\infty , if \{X_n^{(1)}\}$ and $\{X_n^{(2)}\} $ never meet$\\
\end{cases}
$$
$\tau$ is a stopping time for the process $\{Z_n\}$, which is positive recurrent.\\
This implies $P[\tau < \infty] = 1$\\
Consider a process $X_n'$ defined as follows\\
$$
X_n' =
\begin{cases}
X_n^{(1)}, n \leq \tau\\
X_n^{(2)}, n > \tau\\
\end{cases}
$$

\textbf{CLAIM:}\\
${X_n'}_{n \geq 0}$ is a homogenous DTMC with transition matrix $P$ and initial distribution $\mu$. It inherits all the properties of $X_n^{(1)}$.\\
Also, $\tau$ is a coupling time for $\{X_n^{(1)}\}_{n \geq 0}$ and $\{X_n^{(2)}\}_{n \geq 0}$.\\
From the coupling inequality,\\
\begin{eqnarray*}
\begin{aligned}
 \sum_{i \in N_0} \lvert P(X_n' = i) - P(X_n^{(2)} = i) \rvert \leq P[\tau < n]\\
 n \longrightarrow \infty:  P(X_n^{(2)} = i) \longrightarrow \Pi(i),  P[\tau > n] \longrightarrow 0\\
 n \longrightarrow \infty \Rightarrow P(X_n' = i) = \Pi(i) 
\end{aligned}
\end{eqnarray*}
We can get bounds on the rate of convergence by bounding $P[\tau > n]$

\subsection{Example}
Let $X$ and $Y$ be two binomial distributions defined as follows:\\
$X \sim Bin(n,p)$\\
$Y \sim Bin(n,q)$ and $p > q$\\
What is the relation between $P_1[X > k]$ and $P_2[Y > k]$  $\forall$ k?\\
\textbf{Solution:}\\
Consider $n$ Bernoulli random variables, $Z_1,Z_2,....Z_n \sim Ber(p)$\\
Define $W_1,W_2....W_n$ such that $W_i \sim Ber(p*q/p)$ when the event $\{Z_i = 1\}$ which occurs with probability $p$ and $W_i = 0$ when the event $\{Z_i = 0\}$
occurs with probability $q$.\\
$\Rightarrow W_i \sim Ber(q)$ and $Z_i \sim Ber(p)$, with $p > q$ \\
From the definition, we have $\sum_i W_i \leq \sum_i Z_i $\\
$\Rightarrow P[\sum_i W_i > k] \leq  P[\sum_i Z_i > k]$\\
We know that $\sum_i Z_i = X$ and $\sum_i W_i = Y$\\
Therefore, $\forall$ k, $P_1[X > k] \geq P_2[Y>k]$\\

\section{Mean time spent in the transient states}
Consider a finite state space DTMC.'\\
Let the transient states be $\{1,2,...t\}$.\\
\begin{eqnarray*}
	\begin{aligned}
	&	Q_{t \times t} = P_{[t] \times [t]}\\
	&	Q_{t \times t} = \begin{bmatrix} p_{11}&-&-&-&p_{1t}\\ p_{21}&-&-&-&p_{2t} \\ -&-&-&-&-\\ p_{t1}&-&-&-&p_{tt} \end{bmatrix}
    \end{aligned}
\end{eqnarray*}
\textbf{NOTE:} All row sums of $Q$ cannot be equal to 1.\\
Atleast one row should not sum upto 1, else it contradicts the claim that $Q$ is a transition matrix for the set of transient states.\\
For $i,j \in [t]$, let\\
\begin{eqnarray*}
	\begin{aligned}
& m_{ij} = E_i[\sum_{n=0}^{\infty} \textbf{1} \{X_n = j\}]\\
& m_{ij} = \sum_{n=0}^{\infty} p_{ij}^{(n)} < \infty\\
& m_{ij} = \textbf{1}[i = j] + \sum_{k=1}^{[t]} p_{ik}m_{kj} \\
& M = I + QM\\
& M(I - Q) = I\\
& M = (I - Q)^{-1}\\
& M = I + Q + Q^{2} + Q^{3} + ......\\
    \end{aligned}
\end{eqnarray*}
$M$ is called the fundamental matrix.\\
Here, $(I - Q)$ is invertible because all row sums of $Q$ don't sum upto 1.\\
\textbf{Note:}\\
$\forall i,j \in [t]$, consider $f_{ij} = \mathbb{E}_i[\textbf{1} \{\exists n \geq 0: X_n = j \}]$\\
\begin{eqnarray*}
\begin{aligned}
& m_{ij} = \mathbb{E}_i[\# transitions\ to\ j \in \{0,1,2...\infty\}]\\
& m_{ij} = f_{ij} m_{jj}\\
&\Rightarrow i,j \in [t]; f_{ij} = \dfrac{m_{ij}}{m_{jj}}
\end{aligned}
\end{eqnarray*}
 
\end{document}
=======
% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\include{header}
\title{Lecture 12 : Continuous Time Markov Chains}
\author{}
\begin{document}
\maketitle

\section{Markov Process}
\begin{defn}
For a countable set $I$ a continuous time stochastic process $\{X(t) \in I, ~ t \geqslant 0\}$ is a \textbf{Markov process} if
\begin{align*}
\Pr\{X(t+s) = j |X(u),~ u \in [0,s]\} &= \Pr\{X(t+s) = j |X(s)\}, \text{ for all } s, t \geqslant 0 \text{ and } i, j \in I.
\end{align*}
We define the \textbf{transition probability} from state $i$ at time $s$ to state $j$ at time $s+t$ as 
\begin{align*}
P_{ij}(s, s+t) = \Pr\{X(s+t) = j | X(s) = i\}.
\end{align*}
The CTMC has \textbf{homogeneous} transitions if $P_{ij}(s,s+t) = P_{ij}(0,t)$ for all $i,j \in I, s,t \geqslant 0$ and we denote the transition probability by $P_{ij}(t)$. 
\end{defn}

\subsection{Sojourn Times and Jump Transitions}
Suppose $X(0)=i$, and  for all $u \in [0,s]$, we have $X(u)=i$. We are interested in knowing probabilities of the form $\Pr\{X(v)=i,~v \in [s,s+t] | X(u)=i, ~ u \in [0,s]\}$. To this end, we define sojourn times in any state.
\begin{defn}
Sojourn times of a CTMC $\{X(t), t \geqslant 0\}$ is defined by
\begin{align*}
\tau_i \triangleq \{t \geqslant 0: X(t) \neq i|X(0)=i\}, i \in \N_0. 
\end{align*}
\end{defn}
\begin{lem}
For a homogeneous CTMC, sojourn time $\tau_i$ is a continuous memoryless random variable. %and exponentially distributed. 
\end{lem}
\begin{proof}
We observe that,
\begin{align*}
\Pr\{\tau_i \geqslant s+t | \tau_i > s\} &=\Pr\{X(v)=i,~v \in [s,s+t) | X(u)=i,  i \in [0, s]\}\\
&= \Pr\{X(v)=i,~v \in [0,t) | X(0)=i\} = \Pr\{\tau_i \geqslant t \}.
\end{align*}
\end{proof}
We could sample the process at these instants and construct a DTMC and study the same. 
\begin{defn}
Jump transition probabilities of a CTMC $\{X(t), t \geqslant 0\}$ are defined by
\begin{align*}
p_{ij} \triangleq \Pr\{X(\tau_i)  = j | X(0) =  i \}, ~i,j \in \N_0.
\end{align*}
\end{defn}
\begin{lem}
For a CTMC, jump transition probabilities $p_{ij}$ add to unity for all $i \in \N_0$.
\end{lem}
\begin{proof}
\end{proof}

\subsection{Alternative way of constructing CTMC}
\begin{prop} A continuous time stochastic process $\{X(t) \in \N_0, t \geqslant 0 \}$ is a CTMC iff \begin{enumerate}
\item sojourn time $\tau_i$ is distributed with $\exp(\nu_i)$, and 
\item jump transition probabilities $p_{ij}$ is such that $\sum_{i \neq j}p_{ij}=1$.
\end{enumerate}
\end{prop}
\begin{proof}
\end{proof}
\begin{rem}
Transition probabilities $p_{ij}$ and sojourn times $\tau_i$ are independent. 
\end{rem}
\begin{rem} Inverse of mean sojourn time $\nu_i$ is called rate of state $i$, and typically $\nu_i < \infty$.  
\end{rem}
\begin{rem} If $\nu_i = \infty$, we call the state to be instantaneous. 
\end{rem}
\begin{rem}  A CTMC is a DTMC with exponential sojourn time in each state.
\end{rem}
\begin{defn} A CTMC is called \textbf{regular} if 
\begin{align*}
\Pr\{ \text{ number of transitions in } [0,t] \text{ is finite}\} = 1,~ \forall t < \infty.
\end{align*} 
\end{defn}
\begin{exmp} Consider the following example of a non-regular CTMC. $p_{i,i+1}=1, \nu_i = i^2$. Show that it is not regular.
\end{exmp}

\section{Generator Matrix}
\begin{defn}
A \textbf{generator matrix} denoted by $Q$ is defined in terms of sojourn times $\nu_i$ and jump transition probabilities $p_{ij}$ of a CTMC as
\begin{enumerate}
\item {$q_{ij}=\nu_i p_{ij}$, $\forall i \neq j$, } 
\item { $q_{ii}= -\nu_i$.}
\end{enumerate}
\end{defn}
\begin{lem} Generator matrix $Q$ has following properties.
 \begin{enumerate}
\item {$0 \leq -q_{ii} < \infty,~ \forall i$.} 
\item { $q_{ij} \geq 0,~ \forall i \neq j$.}
\item { $\sum_{j}q_{ij}=0,~ \forall i$.}
\end{enumerate}
\end{lem}

From the $Q$ matrix, we can construct the whole CTMC.  In DTMC, we had the result $P^{(n)}(i,j)=(P^n)_{i,j}$. We can generalize this notion  in the case of CTMC as follows: $P=e^{Q}\triangleq \sum_{k \in \mathbb{N}_0}\frac{Q^k}{k !}$.  Observe that $e^{Q_1+Q_2}=e^{Q_1}e^{Q_2},~ e^{nQ}=(e^Q)^n=P^n$.\\
\begin{thm}
Let $Q$ be a finite sized matrix. Let $P(t)=e^{tQ}$. Then $\{P(t),~ t \geq 0\}$ has the following properties:\begin{enumerate}
\item {$P(s+t)=P(s)P(t),~ \forall s,~t$ (semi group property).}
\item {$P(t),~t \geq 0$ is the unique solution to the forward equation, $\frac{dP(t)}{dt}=P(t)Q,~P(0)=I$.}
\item {And the backward equation $\frac{dP(t)}{dt}=QP(t),~P(0)=I$.}\\
\item {For all $k \in \mathbb{N}$, $\frac{d^kP(t)}{d^k(t)}|_{t=0}=Q^k$.}
\end{enumerate}
\end{thm}  
\begin{proof}
$\frac{dM(t)e^{-tQ}}{dt}=0,$ $M(t)e^{-tQ}$ is constant. $M(t)$ is any matrix satisfying the forward equation.
\end{proof}
\begin{thm}
A finite matrix $Q$ is $Q$ matrix if and only if $P(t)=e^{tQ}$ is a stochastic matrix for all $t \geq 0$. 
\end{thm}
\begin{proof}
$P(t)=I+tQ+O(t^2)$ ($f(t)=O(t) \Rightarrow \frac{f(t)}{t} \leq c,$ for small $t,~c < \infty$ ). $q_{ij} \geq 0$ if and only if $P_{ij}(t) \geq 0,~ \forall i \neq j$ and $t \geq 0$ sufficiently small. $P(t)=P(\frac{t}{n})^n$. Note that if $Q$ has zero row sums, $Q^n$ also has zero row sums.\\
\begin{flalign*}
\sum_j [Q^n]_{ij}&= \sum_j \sum_k [Q^{n-1}]_{ik}Q_{kj}= \sum_j \sum_k Q_{kj}[Q^{n-1}]_{ik}=0.\\
\sum_{j}P_{ij}(t)&=1+\sum_{n \in \mathbb{N}} \frac{t^n}{n!}\sum_j [Q^n]_{ij}=1+0=1.
\end{flalign*}  
Conversely $\sum_{j}P_{ij}(t)=1,~ \forall t \geq 0$, then $\sum_jQ_{ij}= \frac{dP_{ij}(t)}{dt}=0$.
\end{proof}
\subsection{Kolmogorov Differential Equations}
\begin{lem}
\begin{enumerate}
\item{$\lim_{t \rightarrow 0} \frac{1-P_{ii}(t)}{t}=\nu_i$.}\\
\item{$\lim_{t \rightarrow 0} \frac{P_{ij}(t)}{t}=q_{ij}$.}
\end{enumerate}
\end{lem}
\begin{lem}
For all $s,~t \geq 0$, $P_{ij}(t+s)=\sum_{k \in \mathbb{N}_0 P_{ik}(t)P_{kj}(s)}$
\end{lem}
\subsection{Chapman Kolmogorov Equation for CTMC}
\begin{thm}
\textbf{Kolmogorov Backward equation:} For all $i,j,t \geq 0$, $P'_{ij}(t)= \sum_{k \neq i}q_{ik}P_{kj}(t)-\nu_iP_{ij}(t)$ ,  $\frac{dP(t)}{dt}=QP(t)$
\end{thm}
\begin{proof}
$P_{ij}(t+h)=\sum_kP_{ik}(h)P_{kj}(t).$
\begin{flalign*}
P_{ij}(t+h)-P_{ij}(t)=\sum_{k \neq 1}P_{ik}(h)P_{kj}(t)-(1-P_{ii}(h))P_{ij}(t), 
\end{flalign*}
divide by $h$, $h \rightarrow 0$, we get $\frac{dP_{ij}(t)}{dt}=\lim_{h \rightarrow 0}P'_{ij}(t)= \sum_{k \neq i}q_{ik}P_{kj}(t)-\nu_iP_{ij}(t)$. Now the exchange of limit and summation has to be justified. 
\begin{flalign*}
&\liminf_{h \rightarrow 0} \sum_{k \neq 1}\frac{P_{ik}(h)}{h}P_{kj}(t) \geq \liminf_{h \rightarrow 0}\sum_{k \neq 1}\frac{P_{ik}(h)}{h}P_{kj}(t), k <N\\
&= \sum_{k \neq 1}q_{jk}P_{kj}(t),~k <N.
\end{flalign*}
This is true for any finite $N$. Take supremum over all $N$. We get \\
$\liminf_{h \rightarrow 0} \sum_{k \neq 1}\frac{P_{ik}(h)}{h}P_{kj}(t) \geq \sum_{k \neq 1}q_{jk}P_{kj}(t)$. Suffices to show that $\limsup_{h \rightarrow 0} \sum_{k \neq 1}\frac{P_{ik}(h)}{h}P_{kj}(t) \leq \sum_{k \neq 1}q_{jk}P_{kj}(t)$. To that end, 
\begin{flalign*}
\text{LHS}& \leq \limsup_{h \rightarrow 0}[\sum_{k \neq 1, k<N}\frac{P_{ik}(h)}{h}P_{kj}(t)+\sum_{k \neq 1, k\geq N}\frac{P_{ik}(h)}{h} ]\\
& = \limsup_{h \rightarrow 0}[\sum_{k \neq 1, k<N}\frac{P_{ik}(h)}{h}P_{kj}(t)+\frac{1-P_{ii}(h)}{h} \sum_{k \neq 1, k\geq N}\frac{P_{ik}(h)}{h} ]\\
& = [\sum_{k \neq 1, k<N}q_{ik}P_{kj}(t)+\nu_i- \sum_{k \neq 1, k\geq N}q_{ik} ]\\
&=\sum_{k \neq 1}q_{ik}P_{kj}(t). 
\end{flalign*}
\end{proof}
\begin{thm}
\textbf{Kolmogorov Forward Equation:} Under suitablle reqularity conditions, $P'_{ij}(t)=\sum_{k \neq i}P_{ik}(t)q_{kj}-P_{ij}(t)\nu_i$, i.e. $\frac{dP(t)}{dt}=P(t)Q$.
\end{thm}
\end{document}
>>>>>>> 3ae3511b97af20b070d525442f60951d2a53ccda
