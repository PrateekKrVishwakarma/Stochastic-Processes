\documentclass[a4paper,10pt]{article}
\usepackage{%
	amsmath,%
	amsfonts,%
	amssymb,%
	amsthm,%
	hyperref,%
	url,%
	latexsym,%
	epsfig,%
	graphicx,%
	psfrag,%
	subfigure,%	
	color,%
	tikz,%
	pgf,%
	pgfplots,%
	pgfplotstable,%
	pgfpages%
}

\usepgflibrary{shapes}
\usetikzlibrary{%
  arrows,%
	backgrounds,%
	chains,%
	decorations.pathmorphing,% /pgf/decoration/random steps | erste Graphik
	decorations.text,%
	matrix,%
  positioning,% wg. " of "
  fit,%
	patterns,%
  petri,%
	plotmarks,%
  scopes,%
	shadows,%
  shapes.misc,% wg. rounded rectangle
  shapes.arrows,%
	shapes.callouts,%
  shapes%
}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{exmp}[thm]{Example}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{note}[thm]{Note}

\date{}

\title{Lecture 4: Renewal Theory}
\author{Parimal Parag}

\begin{document}
\maketitle

\section{Renewal Theory}
One of the characterization for the Poisson process is of it being a counting process with \emph{iid} exponential inter-arrival times. Now we shall relax the ``exponential" part. A counting process with \emph{iid} general inter-arrival times is called a renewal process.
As a result, we no longer have the nice properties such as Independent and stationary increments that Poisson processes had. However, we can still get some great results which also apply to Poisson Processes. 

\begin{defn}[Inter-arrival Times]Let $\{T_i: i \in \mathbb{N}\}$ be a sequence of \emph{iid} random variables with a common distribution $F$. We interpret $T_n$ as the time between $(n - 1)^{\mathrm{st}}$ and the $n^{\mathrm{th}}$ event. Assume 
\begin{enumerate}
  \item Positive inter-arrival time, i.e. $T_n \geq 0$,
	\item Finite mean i.e. $(0 \leq \mu = E[T_1] < \infty)$, and
	\item $F_n(0) = \Pr\{T_n \leq 0\} = \Pr\{T_n = 0\} < 1$.
\end{enumerate}
\end{defn}

\begin{defn}[Event Instants] If we let $S_n$ denote the time of $n^{\mathrm{th}}$ event, and assume $S_0 = 0$. Then, we have
\begin{equation*} 
S_n = \sum_{i=1}^n T_i, \quad n\in \mathbb{N}. 
\end{equation*}
\end{defn}

\begin{defn}[Renewal process] Let $N(t)$ be the counting process that counts number of events by time $t$. Then,
\begin{equation*} 
N(t) = \sup\{n \in \mathbb{N}_0 : S_n \leq t\}.
\end{equation*} 
This counting process $\{N(t), t \geq 0\}$ is called a renewal process.
\end{defn}
\begin{note}[Inverse Relationship]
We note the inverse relationship between time of $n^{\mathrm{th}}$ event $S_n$, and the counting process $N(t)$, we have 
\begin{equation}
\label{eq:InverseRelationship}
\{S_n \leq t\} \iff \{N(t) \geq n\},
\end{equation}
since $N(t) = \sum_{n \in \mathbb{N}}1_{\{S_n \leq t\}}$.
\end{note}

\subsection{Time average of renewals}
We are interested in knowing how many renewals occur per unit time. From SLLN, we have 
\begin{equation*} 
\frac{S_n}{n} \to \mu \quad \mbox{a.s.}
\end{equation*} 
Since $\mu > 0$, we must have $S_n$ growing arbitrarily large as $n$ increases. Thus,
$S_n$ can be finite for at most finitely many $n$. Therefore, $N(t)$ must be finite,
and
\begin{equation*} 
N(t) = \max\{n \in \mathbb{N}_0 : S_n \leq t\}.
\end{equation*} 

\subsection{Distribution of N(t)}
We need to know the distribution of $N(t)$. Denote $F_n = F^{*(n)}$ where $*$ denotes convolution. Essentially, $F^{*(n)}$ is the distribution of $S_n$. We are interested in the following two quantities:
\begin{flalign*}
m(t) &= E[N(t)], \\
M_{N(t)}(\theta) &= E[e^{\theta N(t)}].
\end{flalign*}

From~\eqref{eq:InverseRelationship}, we have 
\begin{equation*}
\Pr\{N(t) = n\} = \Pr\{S_n \leq t\} - \Pr\{S_{n+1} \leq t\} = F_n(t) - F_{n+1}(t).
\end{equation*}
\begin{prop}
\begin{equation*} 
m(t) = \sum_{n \in \mathbb{N}} F_n(t)
\end{equation*}
\end{prop}
\begin{proof}
\begin{equation*}
E[N(t)] = \sum_{n \in \mathbb{N}} \Pr\{N(t) \geq n\} = \sum_{n \in \mathbb{N}} \Pr\{S_n \leq t\} = \sum_{n \in \mathbb{N}} F_n(t)
\end{equation*}
\end{proof}
Alternatively one can prove the same result using indicator functions. Refer Ross for details.

\begin{prop}
\begin{equation*} 
m(t) < \infty \quad \forall 0 \leq t < \infty
\end{equation*}
\end{prop}
\begin{proof}
Since we assumed that $\Pr\{T_n = 0\} < 1$, for some $\alpha > 0$, we have $\Pr\{T_n \geq \alpha\} >0$. Define
\begin{equation*}
\overline{T}_n = \alpha 1_{\{T_n \geq \alpha\}}.
\end{equation*}
Let $\overline{N}(t)$ denote the renewal process with inter-arrival times $\overline{T}_n$. Note that since $T_i$'s are \emph{iid}, so are $\overline{T}_i$ (Why?). In fact, the arrivals now happen at multiples of $\alpha$. And yes, they stack. Moreover, $T_n \geq \overline{T}_n$

The number of arrivals till time $t$, therefore is Geometric with mean $\frac{1}{P[T_n \geq \alpha]}$. Thus 
\begin{equation*}
E[\overline{N}(t)] = \frac{\lceil\frac{t}{\alpha} \rceil + 1}{P[T_n \geq \alpha]} < \infty
\end{equation*}
Since $E[N(t)] \leq E[\overline{N}(t)]$ which follows from $N(t) \leq \overline{N}(t)$, we are done.
\end{proof}
\end{document}