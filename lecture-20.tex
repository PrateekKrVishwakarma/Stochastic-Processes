% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
%\uspackage{tfrupee}
\input{header}
\title{Lecture 19: Random Walks}
\author{}
\begin{document}
\maketitle
\section{Random Walk}
\begin{defn} Let $\{X_i: i \in \N\}$ be \textit{iid} random variables with finite $\E[|X_1|]$. Let
\begin{align*}
S_n &= \sum_{k=1}^n X_i, ~~n \in \N_0.
\end{align*}
Then the process $\{S_n: n \in \N_0\}$ is called a \textbf{random walk}. 
\end{defn}
\begin{defn} A random walk is called a \textbf{simple random walk} if
\begin{align*}
\Pr\{X_1 = 1\} &= 1- \Pr\{X_1 = -1\}.
\end{align*}
\end{defn}
\begin{rem} A simple random walk has the interpretation of the winnings of a gambler who plays a simple coin toss game and wins Rupee 1 if heads and loses Rupee 1 if tails. 
\end{rem}
\begin{rem} Random walks are useful in analyzing GI/GI/1 Queues, Ruin systems and even stock prices.
\end{rem}

\begin{defn} Let $X_i$ belong to probability space $(S, \mathcal{S}, \mu)$. Consider the probability space $(\Omega, \F, P)$  for process $\{X_i: i \in \N\}$ where 
\begin{xalignat*}{5}
&\Omega = \prod_{i \in \N} S, &&\mathcal{F} = \prod_{i \in \N}\mathcal{S},&&P = \prod_{i \in \N}\mu.
\end{xalignat*}
%We define $X_n(\omega) = \omega_n$
%\end{defn}
%\begin{defn} 
A \textbf{finite permutation} of $\N$ is a map $\pi: \N \to \N$ such that $\pi(i) \neq i$ for only finitely many $i$.
\end{defn}
\begin{defn} For a finite permutation $\pi$, we define $(\pi \omega)_i = \omega_{\pi(i)}$ for all $i \in \N$.
\end{defn}
\begin{defn} An event $A$ is \textbf{permutable} if $A = \pi^{-1}A = \{\omega \in \Omega: \pi \omega \in A\}$ for any finite permutation $\pi$.
\end{defn}
\begin{defn} The collection of permutable events is a $\sigma$-field called the \textbf{exchangeable} $\sigma$-field and denoted by $\mathcal{E}$.
\end{defn}
\begin{defn} A sequence $X$ of random variables is called \textbf{exchangeable} if for each $n$ and permutation $\pi: [n] \to [n]$, joint distribution of $(X_1, X_2, \ldots, X_n)$ and $(X_{\pi(1)}, X_{\pi(2)}, \ldots, X_{\pi(n)})$ are same.
\end{defn}
%\begin{defn}
%$X_1, \hdots ,X_n$ is exchangeable if $X_{i_1}, \hdots X_{i_n}$ has the same joint distribution for all permutations $(i_1,i_2 \hdots i_n)$ of $(1, \hdots ,n)$. The infinite sequence of random variables $X_1, X_2 \hdots$ is said to be exchangeable if every finite subsequence $X_1, \hdots ,X_n$ is exchangeable.
%\end{defn}
\begin{exmp}
Suppose balls are selected randomly, without replacement, from an urn consisting of $n$ balls of which $k$ are white. For $i \in [n]$, let
\begin{align*}
   X_i &= 1_{\{ i^{\text{th}}\text{ selection is white}\}},
\end{align*}
then $(X_1, \ldots X_n)$ will be exchangeable but not independent.  
In particular, let $A = \{ i \in [n]: X_i = 1\}$. Then, we know that $|A| = k$, and we can write 
\begin{align*}
\Pr\{X_i = 1, i \in A, X_j = 0, j \in A^c\} = \Pr\{A = (i_1, i_2, \ldots, i_k) \} = \frac{(n-k)!k!}{n!} = \frac{1}{\binom{n}{k}}.
\end{align*}
This joint distribution is independent of set of exact locations $A$, and hence exchangeable. 
Further, we can show that all $X_i$ are identically distributed, since
\begin{align*}
\Pr\{X_1= 1, X_2, \ldots, X_n\} = \Pr\{X_i = 1, X_1, \ldots, X_{i-1}, X_i, \ldots, X_n\}. 
\end{align*}
Further, it can be seen that
\begin{align*}
\Pr\{X_2 = 1|X_1= 1) = \frac{k-1}{n-1} \neq \frac{k}{n-1} = \Pr\{X_2 = 1|X_1 =0\}.
\end{align*}
\end{exmp}
\begin{exmp}
Let $\Lambda$ denote a random variable having distribution $G$. Let $X$ be a sequence of dependent random variables, where each of these random variables are conditionally \textit{iid} with distribution $F_\lambda$ given $\Lambda= \lambda$.Then, these random variables are exchangeable since
\begin{align*}
\Pr\{X_1 \leq x_1 \ldots , X_n \leq x_n\} = \int_{\lambda} \prod_{i=1}^nF_\lambda(x_i)dG(\lambda),
\end{align*}
which is symmetric in $(x_1, \ldots x_n)$. %The are not independent.
\end{exmp}
\begin{thm}
(De Finetti's Theorem) To every infinite sequence of random variables $X_1, X_2 \hdots $ taking values either $0$ or $1$, there corresponds a probability distribution $G$ on $[0,1]$ such that, for all $0 \leq k \leq n$,
\begin{equation*}
\label{De Finetti}
Pr(X_1=X_2= \hdots X_k =1, X_{k+1}= \hdots X_n = 0)= \int_{0}^{1}\lambda^k(1-\lambda)^{n-k}dG(\lambda).
\end{equation*}  
\end{thm}
\begin{proof}
Let $m \geq n $.
\begin{eqnarray*}
&Pr(X_1 = X_2 \hdots X_k =1, X_{k+1}= \hdots X_n 0 )\\
&=\sum_{j=0}^{m}Pr(X_1=\hdots X_k=1, X_{k+1}= X_{n}=0|S_m=j)Pr(S_m=j)\\
&=\sum_{j} \frac{j(j-1) \hdots (j-k+1)(m-j)(m-j-1) \hdots (m-j-(n-k)+1) }{m(m-1) \hdots (m-n+1)}Pr(S_m=j).
\end{eqnarray*}
The last equation follows by exchangeability as given $S_m=j$ each subset of size $j$ of $X_1 \hdots X_m$ is equally likely to be the one consisting of all $1'$s. Letting $S_m=mY_m$, the above equation for large $m$ is roughly equal to $E[Y_m^k(1-Y_m)^{n-k}]$, and the theorem follows letting $m \rightarrow \infty$. Indeed, from a result known as  Helly's theorem it can be shown that for some subsequence $m'$ converging to $\infty$, the distribution of $Y_m'$ will converge to a distribution $G$ and we get
\begin{equation*}
E[Y_{\infty}^k(1-Y_{\infty})^{n-k}] = \int_0^1 \lambda^k(1-\lambda)^{n-k}dG(\lambda).
\end{equation*} 
\end{proof}


\section{Duality in Random Walks}
Essentially, if $X_i$ are iid, then $X_1,X_2,\cdots,X_n$ has the same joint distribution as $X_n, X_{n-1},\cdots, X_1$. The first result we shall show that if $\E[X_1] >0 $ then the random walk will become positive in finite steps. 
\begin{prop}
Suppose $\{X_i: i \in \N\}$ is a sequence of iid random variables with positive mean. Let $S_n = \sum_{k=1}^n X_i$. If 
\begin{align*}N = \min \{n \in \N : S_n > 0\}\end{align*}
Then $\E[N] < \infty$.
\end{prop}
\begin{proof} From duality principle we obtain that
\begin{align*}
\{N > n\} &= \{S_i \leq 0, i \in [n]\} = \left\{\sum_{k=0}^{i-1}X_{n-k} \leq 0, i \in [n]\right\} = \{S_n \leq S_{n-i}, i \in [n]\}.
\end{align*}
It follows that 
\begin{align*}
\E[N] &= \sum_{n \in \N_0} \Pr\{N > n\}
%&= \sum_{n=0}^\infty P[X_1 \leq 0, X_1+X_2 \leq 0, \cdots, X_1+X_2+\cdots+X_n \leq 0]  \label{Dual1}\\
%&= \sum_{n=0}^\infty P[X_n \leq 0, X_n+X_{n-1} \leq 0, \cdots, X_n+X_{n-1}+\cdots+X_1 \leq 0] \label{Dual2}\\
= \sum_{n \in \N_0} \Pr\{S_n \leq S_{n-i}, i \in [n]\}.
\end{align*}
%Where we used the duality principle to get (\ref{Dual2}). 
We define the renewal instants to be when random walk hits a new low. (Why are these renewal instants?) 
Hence, $n$ is a renewal instant after $0$ if $\{S_n \leq S_i: i \in [n]\}$. Hence, we have
\begin{align*}
\E[N] &= \sum_{n\in \N_0}\Pr\{\text{renewal happens at time } n\} = \sum_{n \in \N_0}\Pr\{\text{inter-renewal length} \geq n\}\\
&= 1 + \E[\text{Number of renewals that occur}]
\end{align*}
%
%Now let us count a renewal at time $n$ when $S_n \leq S_{n-1},S_{n} \leq S_{n-2}, \cdots, S_n \leq 0$. Then we get
%\begin{flalign}
%E[N] &= \sum_{n=0}^\infty P[\mbox{renewal happens at time n}]\\
%&= 1 + E[\mbox{No of renewals that occur}]
%\end{flalign}
Since $\E X > 0$, it follows from strong law of large numbers that $S_n \to \infty$. Hence, the expected number of renewals that occur is finite. Thus $\E[N] < \infty$.
\end{proof}
\begin{defn}
The number of distinct values of $(S_0,\cdots, S_n)$ is called \textbf{range}, denoted by $R_n$.
\end{defn}
\begin{prop}
\begin{align*} \lim_{n \in \N} \frac{\E[R_n]}{n} = \Pr\{S_n \neq 0, \forall n \in \N\}\end{align*}
\end{prop}
\begin{proof}
We define indicator function 
\begin{align*}I_k &=1_{\{S_k \neq S_{k-i}, i \in [k]\}}.\end{align*}
Then, we can write range $R_n$ in terms of indicator $I_k$ as 
\begin{align*}R_n = 1 + \sum_{k=1}^nI_k\end{align*}
Let $T = \{n > 0: S_n = 0\}$. Then, $\lim_{k \in \N} \Pr\{T > k\} = \Pr\{S_n \neq 0, \forall n \in \N\}$. 
Further, using the duality principle, we can write
\begin{flalign}
\E[R_n] &= 1 + \sum_{k=1}^n \Pr\{S_i\neq 0, i \in [k]\} 
%&= 1 + \sum_{k=1}^n P[S_1 \neq 0, S_2 \neq 0, \cdots, S_k \neq 0] \\
= \sum_{k=0}^n \Pr\{T > k\}
\end{flalign}
%Where $T$ is the first time the RW returns to 0. 
Result follows by dividing both sides by $n$ and taking limits.
\end{proof}

\begin{thm}[Simple Random Walk] For a simple random walk, where $\Pr\{X_1 = 1\} = p$ the following holds
\begin{align*}
\lim_{n \in \N}\frac{\E[R_n]}{n} &= 
	\begin{cases}
2p - 1, & p > \frac{1}{2}\\
2(1-p)-1, & p <\leq \frac{1}{2}.
	\end{cases}
\end{align*}
\end{thm}
\begin{proof}
%Consider the simple random walk with $\Pr\{X_i = 1\} = p$. 
When $p=\frac{1}{2}$, this random walk is recurrent and thus
\begin{align*}
\Pr\{\text{No Return to }0\} = 0 = \lim_{n \in \N} \frac{\E[R_n]}{n}.
\end{align*}
When $p > \frac{1}{2}$, let $\alpha = \Pr\{\text{return to } 0|X_1 = 1\}$. Since $\E X > 0$, we know that $S_n \to \infty$ and hence $\Pr\{\text{return to } 0|X_1 = -1\} = 1$. We can write unconditioned probability of return of random walk to $0$ as 
\begin{align*}
\Pr\{\text{Return to }0\} = \alpha p+ 1-p.
\end{align*}
Conditioning on $X_2$ yields 
\begin{align*}
\Pr\{S_n = 0 \text{ for some }n|X_1 = 1\} &= p\Pr\{S_n = 0 \text{ for some }n| S_2  = 2\}  + (1-p).
%\Pr\{S_n = 0 \text{ for some }n| S_2 = 0\}\\
\end{align*}
Further noticing that 
\begin{align*}
\Pr\{S_n = 0 \text{ for some }n| S_2  = 2\} &= \alpha \Pr\{S_{n+m} = 0 \text{ for some }n| S_m = 1 \text{ for some }m \},
\end{align*}
we conclude $\alpha = \alpha^2 p + 1-p$. 
Solving for $\alpha$ yields $\alpha = \frac{1-p}{p}$, and hence the result follows. We can show similarly for the case when $p < 1/2$.
%. Thus for $p > 1/2$,
%\begin{align*}\lim_{n \in \N}\frac{\E[R_n]}{n} = 2p-1.\end{align*}
%Similarly, we can show for $p \leq 1/2$, we have
%\begin{align*}\lim_{n \in \N}\frac{\E[R_n]}{n} = 2(1-p)-1.\end{align*}
\end{proof}

\begin{prop}
In the symmetric RW $(p=1/2)$, the expected number of visits to state $k$ before returning to origin is equal to $1$ for all $k \neq 0$.
\end{prop}

\begin{proof}
For $k>0$, let $Y$ denote the number of visits to state k before the first return to origin. Then
\begin{align*}Y = \sum_{n=1}^\infty I_n\end{align*}
where
\begin{align*}I_n = \left\{ \begin{array}{cl}
	1& \mbox{ if a visit is made to state k at time n and}\\
	& \mbox{there is no return to origin before n}\\
	0 & \mbox{ else}
\end{array}\right.\end{align*} 
Thus
\begin{flalign}
E[Y] &= \sum_{n=1}^\infty P[S_n > 0, S_{n-1}> 0, \cdots, S_1>0, S_n = k]\\
&= \sum_{n=1}^\infty P[X_1+X_2 +\cdots X_n > 0, X_2+X_3+\cdots +X_n> 0, \cdots, X_n>0, S_n = k]\\
&= \sum_{n=1}^\infty P[S_n > 0, S_n > S_1, \cdots, S_n > S_{n-1}, S_n=k] \\
&= \sum_{n=1}^\infty P[\mbox{Symmetric RW hits k for first time at time n}] \\
&= P[\mbox{Symmetric RW ever hits k}] = 1 \mbox{ due to recurrence}
\end{flalign}
\end{proof}
\subsection{ GI/GI/1 Queueing Model}
Consider a $GI/GI/1$ queue. Customers arrive in accordance with a renewal process having an arbitrary interarrival distribution  $F$, and the service distribution is $G$. Let the interarrival times be $X_1,~X_2 \hdots $ and let the service times be $Y_1,~Y_2 \hdots$ and let $D_n$ denote the delay in queue of the $n^\text{th}$ arrival. The following recursion for  $D_n$ is easy to verify:

\begin{displaymath}
   D_{n+1} = \left\{
     \begin{array}{lr}
       D_n+Y_n-X_{n+1} & ~\text{if}~ D_n+Y_n \geq X_{n+1} \\
       0 & ~\text{if}~ D_n+Y_n <X_{n+1}
     \end{array}
   \right.
\end{displaymath}
Let $U_n \equiv Y_n-X_{n+1},~ n \geq 1$,
\begin{equation*}
D_{n+1}=\max\{0,D_n+U_n\},~ n \geq 0.
\end{equation*}
Iterating the above relation yields
\begin{eqnarray*}
D_{n+1}&=\max\{0,D_n+U_n\}\\
&=\max\{0,U_n+\max\{0,D_{n-1}+U_{n-1}\}\}\\
&=\max\{0,U_n,U_n+U_{n-1}+D_{n-1}\}\\
\vdots
&=\max\{0,U_n,U_n+U_{n-1}, \hdots U_n+U_{n-1}+\hdots U_1\},
\end{eqnarray*}
where in the last step we have used the fact that $D_1=0$. Hence, for $c>0$,
\begin{eqnarray*}
Pr(D_{n+1} \geq c )&=Pr(\max\{0,U_n,U_n+U_{n-1}, \hdots , U_n+ \hdots +U_1\} \geq c)\\
&=Pr(\max\{0,U_1,U_2+U_{1}, \hdots , U_1+ \hdots +U_n\} \geq c),
\end{eqnarray*}
where the last equality follows from duality. Thus the following proposition holds.
\begin{prop}
\label{prop_gigi1}
If $D_n$ is the delay in the queue of the $n^\text{th}$ customer in a GI/GI/1 queue with interarrival times $X_i,~ i \geq 1$, and service times $_i,~ i \geq 1$ then
\begin{equation}
\label{GIGI1_prop_eqn}
Pr(D_{n+1} \geq c) = Pr(\text{the random walk}~ S_j,~ j \geq 1,~\text{crosses}~ c~ \text{by time}~ n ),
\end{equation}
where 
\begin{equation*}
S_j=\sum_{i=1}^{j}(Y_i-X_{i+1}).
\end{equation*}
\end{prop}
From Proposition \ref{prop_gigi1} that $Pr(D_{n+1} \geq c)$ is nondecreasing in $n$. Let
\begin{equation*}
Pr(D_{\infty} \geq c)=\lim_{n \rightarrow \infty}Pr(D_{n} \geq c),
\end{equation*}
we have from \ref{GIGI1_prop_eqn}
\begin{equation}
\label{random_walk_gigi1}
Pr(D_{\infty} \geq c) = Pr(\text{the random walk}~ S_j,~ j \geq 1,~\text{ever crosses}~ c).
\end{equation}
If $E[U]=E[Y]-E[X]$ is positive, then by Strong Law of Large Numbers (SLLN) the random walk will converge to positive infinity with probability 1. Hence,
\begin{equation*}
Pr(D_{\infty} \geq c) =1,~ \forall c ~\text{if}~ E[Y]>E[X].
\end{equation*}
The above will also be true when $E[Y]=E[X]$ and hence we get that $E[Y]< E[X]$ will imply the existence of a stationary distribution.\\
Let $M_n=\max\{0,S_1,S_2 \hdots S_n\},~ n \geq 1$. We have the following proposition.
\begin{prop}
\textbf{Spitzer's Identity}
\begin{equation*}
E[M_n]= \sum_{k=1}^{n} \frac{1}{k} E[S_k^+].
\end{equation*}
\end{prop}
\begin{proof}
We represent $M_n$ as 
\begin{equation*}
M_n=1_{\{S_n>0\}}M_n+1_{\{S_n \leq 0\}}M_n.
\end{equation*}
Consider first $1_{S_n>0}M_n$.
\begin{equation*}
1_{\{S_n>0\}}M_n= 1_{\{S_n>0\}}\max_{1\leq i \leq n}S_i = 1_{\{S_n>0\}}(X_1+\max\{0,X_2,\hdots X_2+\hdots +X_n\})
\end{equation*}
Taking expectation,
\begin{equation}
\label{sptiz_1}
E[1_{\{S_n>0\}}M_n]= E [ 1_{\{S_n>0\}} X_1 ]+ E[ 1_{\{S_n>0\}} \max\{0,X_2,\hdots X_2+\hdots +X_n\}].
\end{equation}
The joint distribution of $X_1, \hdots X_n$ and $X_n,X_1, \hdots X_{n-1}$ are the same. 
\begin{equation}
\label{sptiz_2}
 E[ 1_{\{S_n>0\}} \max\{0,X_2,\hdots X_2+\hdots +X_n\}]=E[1_{\{S_n >0\}}M_{n-1}].
\end{equation}
Since $X_i,S_n$ has the same joint distribution for all $i$,
\begin{equation*}
E[S_n1_{\{S_n>0\}}]=E[\sum_{i=1}^{n}X_i 1_{\{S_n>0\}}]=nE[X_11_{\{S_n>0\}}].
\end{equation*}
Hence,
\begin{equation}
\label{sptiz_3}
 E[X_11_{\{S_n>0\}}]=\frac{1}{n}=E[S_n 1_{\{S_n>0\}}] =\frac{1}{n}E[S_n^+].
\end{equation}
From equations \ref{sptiz_1}, ~\ref{sptiz_2}, ~ \ref{sptiz_3}, we have that
\begin{equation*}
E[1_{\{S_n>0\}}M_n]=E[1_{\{S_n>0\}}M_{n-1}]+\frac{1}{n}E[S_n^+].
\end{equation*}
Also, $S_n \leq 0$ implies that $M_n=M_{n-1}$, it follows that
\begin{equation*}
1_{\{S_n\leq 0\}}M_n=1_{\{S_n\leq 0\}}M_{n-1}.
\end{equation*}
Thus,
\begin{equation*}
E[M_n]=E[M_{n-1}]+\frac{1}{n}E[S_n^+].
\end{equation*}
Upon recursion, we get
 \begin{equation*}
E[M_n]= \sum_{k=2}^{n} \frac{1}{k} E[S_k^+]+E[M_1].
\end{equation*}
\end{proof}
Since, $M_1 =S_1^+$, the result follows. From Proposition \ref{prop_gigi1}, with $M_n=\max\{0,S_1,\hdots S_n\}$
 \begin{equation*}
Pr(D_{n+1} \geq c) = Pr(M_n \geq c).
\end{equation*}
Hence,
 \begin{equation*}
E[D_{n+1}]=E[M_n].
\end{equation*}
From Spitzer's identity we see that
 \begin{equation*}
E[D_{n+1}]= \sum_{k=1}^{n} \frac{1}{k}E[S_k^+].
\end{equation*}

%\subsection{Some Remarks Concerning Exchangeable Random Variables}
\end{document}