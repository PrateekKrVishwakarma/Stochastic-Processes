\documentclass[a4paper,10pt]{article}
\usepackage{%
amsmath,%
amsfonts,%
amssymb,%
amsthm,%
hyperref,%
url,%
latexsym,%
epsfig,%
graphicx,%
psfrag,%
subfigure,%
color,%
tikz,%
pgf,%
pgfplots,%
pgfplotstable,%
pgfpages%
}
\usepgflibrary{shapes}
\usetikzlibrary{%
arrows,%
backgrounds,%
chains,%
decorations.pathmorphing,% /pgf/decoration/random steps | erste Graphik
decorations.text,%
matrix,%
positioning,% wg. " of "
fit,%
patterns,%
petri,%
plotmarks,%
scopes,%
shadows,%
shapes.misc,% wg. rounded rectangle
shapes.arrows,%
shapes.callouts,%
shapes%
}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{exmp}[thm]{Example}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{note}[thm]{Note}
\date{}
\title{Lecture 20: Random Walks}
\author{Parimal Parag}
\begin{document}
\maketitle
\subsection{ GI/GI/1 Queueing Model}
Consider a $GI/GI/1$ queue. Customers arrive in accordance with a renewal process having an arbitrary interarrival distribution  $F$, and the service distribution is $G$. Let the interarrival times be $X_1,~X_2 \hdots $ and let the service times be $Y_1,~Y_2 \hdots$ and let $D_n$ denote the delay in queue of the $n^\text{th}$ arrival. The following recursion for  $D_n$ is easy to verify:

\begin{displaymath}
   D_{n+1} = \left\{
     \begin{array}{lr}
       D_n+Y_n-X_{n+1} & ~\text{if}~ D_n+Y_n \geq X_{n+1} \\
       0 & ~\text{if}~ D_n+Y_n <X_{n+1}
     \end{array}
   \right.
\end{displaymath}
Let $U_n \equiv Y_n-X_{n+1},~ n \geq 1$,
\begin{equation*}
D_{n+1}=\max\{0,D_n+U_n\},~ n \geq 0.
\end{equation*}
Iterating the above relation yields
\begin{eqnarray*}
D_{n+1}&=\max\{0,D_n+U_n\}\\
&=\max\{0,U_n+\max\{0,D_{n-1}+U_{n-1}\}\}\\
&=\max\{0,U_n,U_n+U_{n-1}+D_{n-1}\}\\
\vdots
&=\max\{0,U_n,U_n+U_{n-1}, \hdots U_n+U_{n-1}+\hdots U_1\},
\end{eqnarray*}
where in the last step we have used the fact that $D_1=0$. Hence, for $c>0$,
\begin{eqnarray*}
Pr(D_{n+1} \geq c )&=Pr(\max\{0,U_n,U_n+U_{n-1}, \hdots , U_n+ \hdots +U_1\} \geq c)\\
&=Pr(\max\{0,U_1,U_2+U_{1}, \hdots , U_1+ \hdots +U_n\} \geq c),
\end{eqnarray*}
where the last equality follows from duality. Thus the following proposition holds.
\begin{prop}
\label{prop_gigi1}
If $D_n$ is the delay in the queue of the $n^\text{th}$ customer in a GI/GI/1 queue with interarrival times $X_i,~ i \geq 1$, and service times $_i,~ i \geq 1$ then
\begin{equation}
\label{GIGI1_prop_eqn}
Pr(D_{n+1} \geq c) = Pr(\text{the random walk}~ S_j,~ j \geq 1,~\text{crosses}~ c~ \text{by time}~ n ),
\end{equation}
where 
\begin{equation*}
S_j=\sum_{i=1}^{j}(Y_i-X_{i+1}).
\end{equation*}
\end{prop}
From Proposition \ref{prop_gigi1} that $Pr(D_{n+1} \geq c)$ is nondecreasing in $n$. Let
\begin{equation*}
Pr(D_{\infty} \geq c)=\lim_{n \rightarrow \infty}Pr(D_{n} \geq c),
\end{equation*}
we have from \ref{GIGI1_prop_eqn}
\begin{equation}
\label{random_walk_gigi1}
Pr(D_{\infty} \geq c) = Pr(\text{the random walk}~ S_j,~ j \geq 1,~\text{ever crosses}~ c).
\end{equation}
If $E[U]=E[Y]-E[X]$ is positive, then by Strong Law of Large Numbers (SLLN) the random walk will converge to positive infinity with probability 1. Hence,
\begin{equation*}
Pr(D_{\infty} \geq c) =1,~ \forall c ~\text{if}~ E[Y]>E[X].
\end{equation*}
The above will also be true when $E[Y]=E[X]$ and hence we get that $E[Y]< E[X]$ will imply the existence of a stationary distribution.\\
Let $M_n=\max\{0,S_1,S_2 \hdots S_n\},~ n \geq 1$. We have the following proposition.
\begin{prop}
\textbf{Spitzer's Identity}
\begin{equation*}
E[M_n]= \sum_{k=1}^{n} \frac{1}{k} E[S_k^+].
\end{equation*}
\end{prop}
\begin{proof}
We represent $M_n$ as 
\begin{equation*}
M_n=1_{\{S_n>0\}}M_n+1_{\{S_n \leq 0\}}M_n.
\end{equation*}
Consider first $1_{S_n>0}M_n$.
\begin{equation*}
1_{\{S_n>0\}}M_n= 1_{\{S_n>0\}}\max_{1\leq i \leq n}S_i = 1_{\{S_n>0\}}(X_1+\max\{0,X_2,\hdots X_2+\hdots +X_n\})
\end{equation*}
Taking expectation,
\begin{equation}
\label{sptiz_1}
E[1_{\{S_n>0\}}M_n]= E [ 1_{\{S_n>0\}} X_1 ]+ E[ 1_{\{S_n>0\}} \max\{0,X_2,\hdots X_2+\hdots +X_n\}].
\end{equation}
The joint distribution of $X_1, \hdots X_n$ and $X_n,X_1, \hdots X_{n-1}$ are the same. 
\begin{equation}
\label{sptiz_2}
 E[ 1_{\{S_n>0\}} \max\{0,X_2,\hdots X_2+\hdots +X_n\}]=E[1_{\{S_n >0\}}M_{n-1}].
\end{equation}
Since $X_i,S_n$ has the same joint distribution for all $i$,
\begin{equation*}
E[S_n1_{\{S_n>0\}}]=E[\sum_{i=1}^{n}X_i 1_{\{S_n>0\}}]=nE[X_11_{\{S_n>0\}}].
\end{equation*}
Hence,
\begin{equation}
\label{sptiz_3}
 E[X_11_{\{S_n>0\}}]=\frac{1}{n}=E[S_n 1_{\{S_n>0\}}] =\frac{1}{n}E[S_n^+].
\end{equation}
From equations \ref{sptiz_1}, ~\ref{sptiz_2}, ~ \ref{sptiz_3}, we have that
\begin{equation*}
E[1_{\{S_n>0\}}M_n]=E[1_{\{S_n>0\}}M_{n-1}]+\frac{1}{n}E[S_n^+].
\end{equation*}
Also, $S_n \leq 0$ implies that $M_n=M_{n-1}$, it follows that
\begin{equation*}
1_{\{S_n\leq 0\}}M_n=1_{\{S_n\leq 0\}}M_{n-1}.
\end{equation*}
Thus,
\begin{equation*}
E[M_n]=E[M_{n-1}]+\frac{1}{n}E[S_n^+].
\end{equation*}
Upon recursion, we get
 \begin{equation*}
E[M_n]= \sum_{k=2}^{n} \frac{1}{k} E[S_k^+]+E[M_1].
\end{equation*}
\end{proof}
Since, $M_1 =S_1^+$, the result follows. From Proposition \ref{prop_gigi1}, with $M_n=\max\{0,S_1,\hdots S_n\}$
 \begin{equation*}
Pr(D_{n+1} \geq c) = Pr(M_n \geq c).
\end{equation*}
Hence,
 \begin{equation*}
E[D_{n+1}]=E[M_n].
\end{equation*}
From Spitzer's identity we see that
 \begin{equation*}
E[D_{n+1}]= \sum_{k=1}^{n} \frac{1}{k}E[S_k^+].
\end{equation*}

\subsection{Some Remarks Concerning Exchangeable Random Variables}
\begin{defn}
$X_1, \hdots ,X_n$ is exchangeable if $X_{i_1}, \hdots X_{i_n}$ has the same joint distribution for all permutations $(i_1,i_2 \hdots i_n)$ of $(1, \hdots ,n)$. The infinite sequence of random variables $X_1, X_2 \hdots$ is said to be exchangeable if every finite subsequence $X_1, \hdots ,X_n$ is exchangeable.
\end{defn}
\begin{exmp}
Suppose balls are selected randomly, without replacement, from an urn consisting of $n$ balls of which $k$ are white. If we let
\begin{displaymath}
   X_1 = \left\{
     \begin{array}{lr}
       1 & ~\text{if}~ i^{\text{th}}~ \text{selection is white} \\
       0 & ~\text{otherwise},
     \end{array}
   \right.
\end{displaymath}
then $X_1, \hdots X_n$ will be exchangeable but not independent. 
\end{exmp}
\begin{exmp}
Let $\Lambda$ denote a random variable having distribution $G$. Given that $\Lambda= \lambda$, $X_1, X_2 \hdots$ are $iid$ with distribution $F_\lambda$. The random variables are exchangeable since
\begin{equation*}
Pr(X_1 \leq x_1 \hdots , X_n \leq x_n) = \int \Pi_{i=1}^nF_\lambda(x_i)dG(\lambda),
\end{equation*}
which is symmetric in $(x_1, \hdots x_n)$. The are not independent.
\end{exmp}
\begin{thm}
(De Finetti's Theorem) To every infinite sequence of random variables $X_1, X_2 \hdots $ taking values either $0$ or $1$, there corresponds a probability distribution $G$ on $[0,1]$ such that, for all $0 \leq k \leq n$,
\begin{equation*}
\label{De Finetti}
Pr(X_1=X_2= \hdots X_k =1, X_{k+1}= \hdots X_n = 0)= \int_{0}^{1}\lambda^k(1-\lambda)^{n-k}dG(\lambda).
\end{equation*}  
\end{thm}
\begin{proof}
Let $m \geq n $.
\begin{eqnarray*}
&Pr(X_1 = X_2 \hdots X_k =1, X_{k+1}= \hdots X_n 0 )\\
&=\sum_{j=0}^{m}Pr(X_1=\hdots X_k=1, X_{k+1}= X_{n}=0|S_m=j)Pr(S_m=j)\\
&=\sum_{j} \frac{j(j-1) \hdots (j-k+1)(m-j)(m-j-1) \hdots (m-j-(n-k)+1) }{m(m-1) \hdots (m-n+1)}Pr(S_m=j).
\end{eqnarray*}
The last equation follows by exchangeability as given $S_m=j$ each subset of size $j$ of $X_1 \hdots X_m$ is equally likely to be the one consisting of all $1'$s. Letting $S_m=mY_m$, the above equation for large $m$ is roughly equal to $E[Y_m^k(1-Y_m)^{n-k}]$, and the theorem follows letting $m \rightarrow \infty$. Indeed, from a result known as  Helly's theorem it can be shown that for some subsequence $m'$ converging to $\infty$, the distribution of $Y_m'$ will converge to a distribution $G$ and we get
\begin{equation*}
E[Y_{\infty}^k(1-Y_{\infty})^{n-k}] = \int_0^1 \lambda^k(1-\lambda)^{n-k}dG(\lambda).
\end{equation*} 
\end{proof}
\end{document}