\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 02: Poisson Process}
\author{}

\begin{document}
\maketitle

\section{Simple point processes}
\begin{defn} A stochastic process $\{N(t), t\geqslant 0\}$ is a \textbf{point process} if
\begin{enumerate}
  \item $N(0) = 0$, and 
  \item for each $\omega \in \Omega$, the map $t\mapsto N(t)$ is non-decreasing, integer valued, and right continuous.% and at points of discontinuity $(N(t)- N(t^{-}))\leqslant 1,~~\forall \omega \in \Omega$. 
\end{enumerate}
\end{defn} 
\begin{defn} A \textbf{simple point process} is a point process of jump size 1.
\end{defn}

\begin{figure}[hhhh]
\center
	\input{Figures/Poisson}
	\caption{Sample path of a simple point process.}
	\label{Fig:Poisson}
\end{figure}
\begin{defn} We can define a random variable $S_n$ as the time of $n^{\text{th}}$ discontinuity, written
\begin{xalignat*}{3}
&S_n = \inf\{t \geq 0: N(t) = n\}, n \in \N,&& S_0 = 0.
\end{xalignat*}
The points of discontinuity corresponds to the arrival instants of the point process. 
\end{defn}
\begin{lem} Simple point process $\{N(t), t \geqslant 0\}$ and arrival process $\{S_n: n \in \N\}$ are inverse processes. That is,
\begin{align*}
\{S_n \leqslant t\} = \{N(t) \geqslant n\}.
\end{align*}
\end{lem}
\begin{proof} Let $\omega \in \{S_n \leqslant t\}$, then $N(S_n) = n$. Since $N$ is a non-decreasing process, we have $N(t) \geq N(S_n) = n$. 
%It follows that $\{S_n \leqslant t\} \subseteq \{N(t) \geqslant n\}$.
Conversely, let $\omega \in \{N(t) \geqslant n\}$, then it follows from definition that $S_n \leq t$.
\end{proof}
\begin{cor} The following identity is true.
\begin{align*}
\{S_n \leqslant t, S_{n+1} > t\} = \{N(t) = n\}.
\end{align*}
\end{cor}
\begin{lem}
Let $F_n(x)$ be the distribution function for $S_n$, then 
\begin{align*}
P_n(t) \triangleq \Pr\{N(t) = n\} = F_{n}(t)-F_{n+1}(t).
\end{align*}
\end{lem}
\begin{proof} It suffices to observe that following is a union of disjoint events,
\begin{align*}
\{S_n \leqslant t, S_{n+1} > t\} \cup \{S_{n} \leqslant t, S_{n+1} \leqslant t\} = \{S_n \leqslant t \}.
\end{align*}
\end{proof}
\begin{defn} The inter arrival time between $(n-1)^{th}$ and $n^{th}$ arrival is denoted by $X_n$ and written as
\begin{align*}
X_n = S_n - S_{n-1}.
\end{align*}
\end{defn}
\begin{rem} For a simple point process, we have
\begin{align*}
\Pr\{X_{n} = 0\} &= \Pr\{X_n\leqslant 0\} = 0.
\end{align*}
\end{rem}
\begin{defn} A point process $\{N(t), t\geqslant 0\}$ is called \textbf{stationary increment point process}, if for any collection of $0 < t_{1}<t_{2}, \ldots,<t_{n}$, the joint distribution of $(N(t_{n})-N(t_{n-1}),N(t_{n-1})-N(t_{n-2}),...,N(t_{1}))$ is identical to the joint distribution of $(N(t_{n}+t)-N(t_{n-1}+t),...,N(t_{1}+t)), ~ \forall t \geqslant 0$.
\end{defn}

\begin{defn} A point process $\{N(t), t\geqslant 0\}$ is called \textbf{stationary independent increment process}, if it has stationary increments and the increments are independent random variables.
\end{defn}

\begin{lem} Sequence of inter-arrival times $\{X_n: n \in \N\}$ of a simple stationary independent increment process $\{N(t), t \geqslant 0 \}$ consists of \emph{iid} random variables.
\end{lem}
\begin{proof} It suffices to show that $X_n$ is independent of $S_{n-1}$ to show that all inter-arrival times are independent. 
We see that
%\begin{align*}
%\{S_n \leqslant x, S_{n+1} - S_n > y\} &= \{S_n \leqslant x, N(y+S_n) - N(S_n) = 0\},\\
%&=\{n = N(S_n) \leqslant N(x), N(y+S_n)-N(S_n)\}. 
%\end{align*}
%Further, we observe that
\begin{align*}
\Pr\{S_n \leqslant x, S_{n+1} - S_n > y\} &= \int_{x \leq t}\Pr\{N(y+t) - N(t) = 0|S_n = t\}dF_n(t)\\
&=\int_{x \leq t}\Pr\{N(y+t) - N(t) = 0|N(t) = n \}dF_n(t) = \Pr\{X_n > y\}F_n(x).
\end{align*}
To show that each inter-arrival time is identically distributed, we observe that
\begin{align*}
\Pr\{S_n - S_{n-1} > x\} &= \Pr\{N(x + S_{n-1}) - N(S_{n-1}) > 0\}\\
&= \int_{t > 0}\Pr\{N(x) = 0\}dF_{n-1}(t) = \Pr\{N(x) = 0\}.
\end{align*}
\end{proof}

\section{Poisson process}
\begin{lem} A unique non-negative right continuous function $f: \R \to \R$ satisfying equation 
\begin{align*}
 f(t+s) = f(t)f(s), \text{ for all } t,s \in \R
 \end{align*}
 is $f(t) = e^{\theta t}$, where $\theta = \log f(1)$.
\end{lem}
\begin{proof}
Clearly, we have $f(0) = f^2(0)$. Since $f$ is non-negative, it means $f(0) = 1$. By definition of $\theta$ and induction for $m,n \in \Z^+$, we see that 
\begin{xalignat*}{3}
&f(m) = f(1)^m = e^{\theta m},&& e^{\theta} = f(1) = f(1/n)^n.
 \end{xalignat*}
Let $q \in \Q$, then it can be written as $m/n, n \neq 0$ for some $m,n \in \Z^+$. 
Hence, it is clear that for all $q \in \Q^+$, we have $f(q) = e^{\theta q}$.
either unity or zero. Note, that $f$ is a right continuous function and is non-negative. 
%We will show that $g$ is an exponential function. That is, $g(t) = e^{\theta t}$ for some $\theta \geqslant 0$. We will prove this in stages. First, we show this is true for $t \in \mathbb{Z}^+$. Notice that we can obtain via induction
%\begin{eqnarray*}
%	g(2) &=& g(1) g(1) = g^{2}(1), \mathrm{ and }\\
%	g(m) &=& [g(1)]^{m}.
%\end{eqnarray*}
%Since $g(1)$ is non negative, there exists a $\beta$ such that $g(1)=e^{\beta}$ and $g(m)= e^{m \beta}, m \in \mathbb{Z}_{+}$. Next we show that for any $n \in \mathbb{Z}_{+}$,        
%\begin{equation*}
%	g(1) =  g\left(\frac{1}{n}+..., +\frac{1}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{n}.
%\end{equation*}
%Therefore, for same $\beta$ we used for $g(1)$, we have $g\left(\frac{1}{n}\right) = e^{\frac{\beta}{n}}$. Now, we show that $g$ is exponential for any $t \in \mathbb{Q}^+$. To this end, we see that for any $m, n \in \mathbb{Z}_{+}$, we have 
%\begin{equation*}
%	g\left(\frac{m}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{m}= e^{\frac{m \beta}{n}}.
%\end{equation*}
Now, we can show that $f$ is exponential for any real positive $t$ by taking a sequence of rational numbers $\{t_n\}$ decreasing to $t$. From right continuity of $g$, we obtain 
\begin{equation*}
	g(t) = \lim_{t_n \downarrow t} g(t_n) =   \lim_{t_n \downarrow t} e^{\beta t_{n}}= e^{\beta t}.
\end{equation*}
\end{proof}
\begin{defn} A random variable $X$ with continuous support on $\R_+$, is called \textbf{memoryless} if for all $t,s \in \R_+$, we have
\begin{align*}
  \Pr\{X>s\} &= \Pr\{ X > t+s| X>t\}.% = \frac{ \Pr\{ X>t+s, X>t\}}{\Pr\{X>t\}}.
\end{align*}
\end{defn}
\begin{prop} The unique memoryless distribution function with continuous support on $\R_+$ is the exponential distribution.
\end{prop}
\begin{proof}
Let $X$ be a random variable with a distribution function $F: \R_+ \to [0,1]$ with the memoryless property. Let $g(t) \triangleq 1 - F(t)$. It follows from the memoryless property of $F$, that
\begin{align*}
 g(t+s) = g(t)g(s).
 %\Pr\{X>s\} &= \Pr\{ X > t+s| X>t\} \\&= \frac{ \Pr\{ X>t+s, X>t\}}{\Pr\{X>t\}}.
\end{align*}
%Since $\{X > t + s\} =\{ X>t+s, X>t\}$, we have $g(t+s) = g(t)g(s)$ and hence $g(0) = g^2(0)$. Therefore, $g(0)$ is either unity or zero. Note, that $g$ is a right continuous function and is non-negative. 
%
%We will show that $g$ is an exponential function. That is, $g(t) = e^{\theta t}$ for some $\theta \geqslant 0$. We will prove this in stages. First, we show this is true for $t \in \mathbb{Z}^+$. Notice that we can obtain via induction
%\begin{eqnarray*}
%	g(2) &=& g(1) g(1) = g^{2}(1), \mathrm{ and }\\
%	g(m) &=& [g(1)]^{m}.
%\end{eqnarray*}
%Since $g(1)$ is non negative, there exists a $\beta$ such that $g(1)=e^{\beta}$ and $g(m)= e^{m \beta}, m \in \mathbb{Z}_{+}$. Next we show that for any $n \in \mathbb{Z}_{+}$,        
%\begin{equation*}
%	g(1) =  g\left(\frac{1}{n}+..., +\frac{1}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{n}.
%\end{equation*}
%Therefore, for same $\beta$ we used for $g(1)$, we have $g\left(\frac{1}{n}\right) = e^{\frac{\beta}{n}}$. Now, we show that $g$ is exponential for any $t \in \mathbb{Q}^+$. To this end, we see that for any $m, n \in \mathbb{Z}_{+}$, we have 
%\begin{equation*}
%	g\left(\frac{m}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{m}= e^{\frac{m \beta}{n}}.
%\end{equation*}
%Now, we can show that $g$ is exponential for any real positive $t$ by taking a sequence of rational numbers $\{t_n\}$ decreasing to t. From right continuity of $g$, we obtain 
%\begin{equation*}
%	g(t) \stackrel{(a)}{=} \lim_{n\rightarrow \infty} g(t_n) =   \lim_{n\rightarrow \infty} e^{\beta t_{n}}= e^{\beta t}.
%\end{equation*}
Since $g(x) = \Pr\{X > x\}$  is non-increasing  with $x \in \R_+$, we have $g(x) = e^{\theta x}$, where $\theta < 0$.
\end{proof}

\begin{defn} A simple point process $\{N(t),~ t\geqslant 0\} $ is called a \textbf{Poisson process} with a finite positive rate $\lambda$, if inter-arrival times $\{X_{n}: n \in \N\}$ are \emph{iid} random variables with an exponential distribution of rate $\lambda$. That is, it has a distribution function $F$, such that 
 \begin{equation*}
 F(x) = \Pr\{X_{1}\leqslant x\} = 
	\begin{cases}
		1-e^{-\lambda x}, & x\geqslant 0   \\
		0,  & \text{ else}.
	\end{cases}
\end{equation*}
\end{defn}

\begin{thm} A simple stationary independent increment process is a Poisson process with parameter $\lambda$ when
\begin{xalignat*}{3}
&\lim_{t \to 0}\frac{\Pr\{N(t) = 1\}}{t} = \lambda,&&\lim_{t \to 0}\frac{\Pr\{N(t) \geq 2\}}{t} = 0.
\end{xalignat*}
\end{thm}
\begin{proof}
It suffices to show that first inter-arrival times $X_1$ is exponentially distributed with parameter $\lambda$. Notice that
\begin{align*}
P_0(t+s) = \Pr\{N(t+s) - N(t) = 0, N(t) = 0\} = P_0(t)P_0(s).
\end{align*}
Using the conditions in the theorem, the result follows.
\end{proof}
\subsection{Distribution functions}
\begin{lem} Moment generating function of arrival times $S_n$ is 
 \begin{equation*}
  \mathbb{E} [ e^{\theta S_n} ] = 
		\begin{cases}
		\frac{\lambda^n}{(\lambda-\theta)^n}, & \theta < \lambda \\
		\infty, & \theta \geqslant \lambda.
		\end{cases} 
 \end{equation*} 
 Distribution function of $S_n$ is given by 
 \begin{align*}.
 \end{align*}
\end{lem}
\begin{proof} 
Since $S_n = \sum_{k=1}^nX_k$, where $X_k$ are \emph{iid}, the moment generating function $\mathbb{E} [ e^{\theta S_{n}} ]$ of $S_n$ is 
 \begin{equation*}
  \mathbb{E} [ e^{\theta S_{n}} ] = \left(\mathbb{E}[e^{\theta X_{1}}]\right)^{n}. 
 \end{equation*} 
Since each $X_k$ is \emph{iid} exponential with rate $\lambda$, it is easy to see that moment generating function of inter-arrival time $X_1$ is 
 \begin{equation*}
  \mathbb{E} [ e^{\theta X_1} ] = 
		\begin{cases}
		\frac{\lambda^n}{(\lambda-\theta)^n}, & \theta < \lambda \\
		\infty, & \theta \geqslant \lambda.
		\end{cases} 
 \end{equation*} 
\end{proof}

\begin{thm} Density function of $S_n$ is Gamma distributed with parameters $n$ and $\lambda$. That is,
\begin{equation*}
f_{n}(s) =\frac{\lambda (\lambda s)^{n-1}} {(n-1)!} e^{-\lambda s}.
\end{equation*}
\end{thm}
\begin{proof} Notice that $X_i$'s are \emph{iid} and $S_1 = X_1$. In addition, we know that $S_n = X_n + S_{n-1}$. Since, $X_n$ is independent of $S_{n-1}$, we know that distribution of $S_n$ would be convolution of distribution of $S_{n-1}$ and $X_1$. Since $X_n$ and $S_1$ have identical distribution, we have $f_{n}=f_{n-1}*f_{1}$. The result follows from straightforward induction.
\end{proof}

%Process $N(t)$ is of real interest, and we can compute the distribution of $N(t)$ for each $t$ from the distribution of $S_n$ in the following.
%\begin{figure}[hhhh]
%\center
	%\include{Figures/Poisson}
 %% \caption{}\label{}
%\end{figure}
\begin{thm} For each $t >0$, the distribution of Poisson process $N(t)$ with parameter $\lambda$ is given by
	\begin{equation*}
	\Pr\{N(t)=n)\}= e^{-\lambda t}\frac{(\lambda t)^{n}}{n!}.
	\end{equation*}
\end{thm}
\begin{proof}
Result follows from density of $S_n$ and recognizing that 
\begin{align*}
P_n(t) = F_n(t) - F_{n+1}(t).
\end{align*}
%\begin{eqnarray*}
%   \Pr\{N(t) =n\}&=&  \Pr\{S_{n}\leqslant t, S_{n+1} >t)\\
%   &=&  \int^{t}_{0} \Pr\left\{ {S_{n+1}>t}|{S_{n}=s}\right\}f_{n}(s)  ds\\
%   &\stackrel{(a)}{=}& \int^{t}_{0} \Pr\{X_{n+1}>t-s\} f_{n}(s) ds\\
%   &=&  \int^{t}_{0}e^{-\lambda(t- s)} \frac{\lambda^{n}s^{n-1}}{(n-1)!}e^{-\lambda s}  ds\\
%   &=&\frac{e^{-\lambda t} (\lambda t)^{n}}{n !}.
%\end{eqnarray*}
% where (a) follows from the memoryless property of exponential distribution. %($\Pr\{X_{n+1}>s+t|X_{k+1}>t)=\Pr\{X_{k+1}>s)$).\\
\end{proof}

\begin{rem} A Poisson process is not a stationary process. That is, the finite dimensional distributions are not shift invariant. 
\end{rem}
%In the following section, we show that the Poisson process is a \textit{stationary,  independent increment} process. To this end, we will use an important property of exponential distribution- namely memoryless property. Memoryless property of exponential distribution will facilitate the computation of fdd of the Poisson process via one dimensional marginal distribution. 
\begin{lem} For any finite time $t > 0$, a Poisson process is finite almost surely.
\end{lem}
\begin{proof} By strong law of large numbers, we have 
\begin{equation*}
\lim_{n \to \infty} \frac{S_{n}}{n} = E[X_{1}] = \frac{1}{\lambda}\quad\mathrm{a.s.} 
\end{equation*}
%Therefore, we have $S_n \rightarrow \infty$, a.s. This implies $\Pr\{N(t) < \infty\} =1$. To see this, 
Fix $t > 0$ and let $M = \{\omega \in \Omega: N(t)(\omega) = \infty \}$ be a subset of the sample space. Let $\omega \in M$, then $S_{n}(\omega)\leqslant t$ for all $n \in \N$. This implies $\lim\sup_n\frac{S_{n}}{n} = 0$  and $\omega \not\in \{\lim_n \frac{S_{n}}{n} = \frac{1}{\lambda} \}.$ Hence, the probability measure for set $M$ is zero. 
\end{proof}

\begin{figure}[hhhh]
\center
	\input{Figures/IndependentIncrements}
  \caption{Stationary independent increment property of Poisson process.}
	\label{Fig:IndependentIncrements}
\end{figure}
\begin{prop} A Poisson process $\{N(t), t\geqslant 0\}$ has stationary independent increments.
\end{prop}
\begin{proof}
To show that $N(t)$ has stationary independent increment property, it suffices to show that $N_t-N(t_{1}) \perp N(t_1)$ and $N(t) - N(t_1) \sim N(t-t_1)$. Since, we can use induction to show this stationary independence increment property for for any finite disjoint time-intervals. The memoryless property of exponential distribution is crucially used. And, we see that independent increment holds only if inter-arrival time is exponential. We can see in Figure~\ref{Fig:IndependentIncrements} that $t_1$ divides $X_{n+1}$ in two parts such that, $X_{n+1} = X_{n+1}^{'} + X_{n+1}^{''}$. Here,  $X_{n+1}^{''}$ is independent of $X_{n+1}^{'}$ and has same distribution as $X_{n+1}$. Therefore, 
\begin{align*}
\{ N(t_1) = n \} &\iff  \{ S_n = t_1 + X'_{n+1} \}, \\
\{ N(t) - N(t_1) \geqslant m \} &\iff \{ X''_{n+1} + \sum_{i=n+2}^{n+m} X_i \leqslant t - t_1 \}.
\end{align*}
Since, $\{X_i: i \geqslant n+2\}\cup\{X_{n+1}^{''}\}$ are independent of $\{X_i: i \leqslant n\}\cup{X_{n+1}^{'}}$, we have $N(t)-N(t_{1}) \perp N(t_1)$. Further, since $X_{n+1}^{''}$ has same distribution as $X_{n+1}$, we get $N(t) - N(t_1) \sim N(t-t_1)$. By induction we can extend this result to $(N(t_{n})-N(t_{n-1}),...,N(t_{1}))$. 
\end{proof}

\begin{prop} Let $t_0 = 0$, and $\{t_i: 1 \leq i \leq k\}$ be an increasing sequence. A stationary independent increment point process $\{N(t),~t\geqslant 0\}$, such that $N(0) = 0$ is Poisson process if 
%\begin{figure}[h!]
%\center
  %% Requires \usepackage{graphicx}
  %\includegraphics[width=2.8in, height=0.9in]{Figures/SPQT.png}\\
 %% \caption{}\label{}
%\end{figure}
\begin{equation*}
  \Pr\{\bigcap_{i=1}^k \{N(t_i)-N(t_{i-1})= n_{i}\}\} = \prod_{i=1}^{k}\frac{(\lambda(t_{i}-t_{i-1}))^{n_{i}}}{n_{i}!} e^{-\lambda (t_{i}-t_{i-1})}.
\end{equation*}
\end{prop}
\end{document}
\section{Characterizations of the Poisson Process}

In the previous section, we have shown that Poisson process has stationary, independent increment property. Now, consider any point process with independent stationary increments. From the discussion in the previous section, it follows that the inter-arrival times have to be \emph{iid} with exponential distribution. Such a characterization does not exclude the possibility of more than one arrival at any time instant. Additionally, if we constrain jump sizes to be unity along with stationary independent increments, the process will be a Poisson process. There are stochastic processes which are stationary independent increment and not Poisson. For example, batch/compound Poisson point process and Brownian motion. We have three alternative characterization of the Poisson processes below. 

\begin{defn}[SII and Joint Distribution]\label{defn:SIIJoint} Let $t_0 = 0$, and $\{t_i: 1 \leq i \leq k\}$ be an increasing sequence. A stationary independent increment point process $\{N(t),~t\geqslant 0\}$, such that $N(0) = 0$ is Poisson process if 
%\begin{figure}[h!]
%\center
  %% Requires \usepackage{graphicx}
  %\includegraphics[width=2.8in, height=0.9in]{Figures/SPQT.png}\\
 %% \caption{}\label{}
%\end{figure}
\begin{equation*}
  \Pr\{\bigcap_{i=1}^k \{N(t_i)-N(t_{i-1})= n_{i}\}\} = \prod_{i=1}^{k}\frac{(\lambda(t_{i}-t_{i-1}))^{n_{i}}}{n_{i}!} e^{-\lambda (t_{i}-t_{i-1})}.
\end{equation*}
\end{defn}

\begin{defn}[SII and Marginal Distribution]\label{defn:SIIMarginal} A point process $\{N(t),~t\geqslant 0\}$ is said to be Poisson process with rate $\lambda$ if it has stationary independent increments, and 
\begin{equation*}
\Pr\{N(t)=n\}= \frac{(\lambda t)^{n}}{n!} e^{-\lambda t}, n\in \mathbb{Z}^+.
\end{equation*}
\end{defn}

\begin{defn}[SII and Infinitesimal Arrivals]\label{defn:SIIInfinitesimal} A point process $\{N(t),~t\geqslant 0\}$ is said to be Poisson process with rate $\lambda$ if it has stationary independent increments, and 
\begin{eqnarray*}\label{eqn1}
\Pr\{N(t)=0\} &=& 1-\lambda t + o(t), \\
  \Pr\{N(t)=1\} &=& \lambda t + o(t), \\
  \Pr\{N(t)>1\} &=& o(t).
\end{eqnarray*}
\end{defn}

\begin{thm}[Equivalent Characterizations] Definitions~\ref{defn:SIIJoint},~\ref{defn:SIIMarginal},~\ref{defn:SIIInfinitesimal} are equivalent to definition of Poisson process.
\end{thm}
\begin{proof}
We have already shown that Poisson process satisfies the conditions in Definitions~\ref{defn:SIIJoint},~\ref{defn:SIIMarginal}. That is, Definition~\ref{defn:SIIJoint} follows from original definition of Poisson process. It is easy to see that Definition~\ref{defn:SIIMarginal} follows trivially from Definition~\ref{defn:SIIJoint}. It is easy to see that Definition~\ref{defn:SIIMarginal} implies Definition~\ref{defn:SIIInfinitesimal}. We will show that Definition~\ref{defn:SIIInfinitesimal} implies original definition of Poisson process. Hence, we would have shown equivalence of all characterizations.
%will now that 
%We first show that Poisson process satisfy the above set of equations. We have shown that the Poisson process has stationary, independent increment property  and 
%\begin{equation*}
  %\Pr\{N(t)= n\} =  e^{-\lambda t} \frac{(\lambda t)^{n}}{n!}, n\in \mathbb{Z}^+.
%\end{equation*}
  %\begin{eqnarray*}
  %\Pr\{N(t)=0\} &=& e^{-\lambda t} = 1-\lambda t + o(t).\\
   %\Pr\{N(t)=1\}&=& e^-\lambda t (\lambda t) \\
  %&=& e^-\lambda t^{(1-e^-\lambda t+ o(t))}\\
   %&=& \lambda t- \lambda^{2} t^{2}+ o(t) \lambda t.
   %\end{eqnarray*}
   %Since $\frac{\lambda^{2} t^{2}}{t} =  \lambda^{2} t$ and $\lim _{t\downarrow 0}\lambda^{2} t= 0$, $\lambda^{2} t^{2} =o(t)$. Consider  $\lambda t o(t) = 0$.   $\lim _{t\downarrow 0}\frac{\lambda t o(t)}{t} = \lambda \lim _{t\downarrow 0} o(t)$. Since sum of two $o(t)$ terms is again an $o(t)$ term, $\Pr\{N(t)=1]= \lambda t + o(t)$. Since the three probability terms given should sum up to 1, $ \Pr\{N(t)>1]=o(t)$. So, we showed that the  Poisson Process has the above stated property. 	Now, we show the converse. 
	
To show the converse, it suffices to show that the time to first jump $X_{1}$ is exponentially distributed with rate $\lambda$. To this end, let $f(t) = \Pr\{X_{1}>t\}$ for $t \geq 0$. It is clear that for any arbitrary $s>0$, we have
\begin{equation*}
\{X_{1}>t+s\}\iff \{N(t)=0\}\cap\{N(t+s)-N(t)=0\}.
\end{equation*}
Due to independent increment property of $N(t)$, we know that $N(t)$ and $N(t+s)-N(t)$ are independent. Further, due to stationarity of the increments, $N(t+s) - N(t)$ has same distribution as $N(s)$. Therefore, we can write
\begin{equation*}
 f(t+s) =   \Pr\{X_{1}>t+s\} = \Pr\{X_{1}>t\} \Pr\{X_{1}>s\} = f(s)f(t).
\end{equation*}
Since, $f$ is right continuous non-negative function with such a property, $f(t) = \exp(-\alpha t)$ for some $\alpha > 0$ and all $t \geq 0$, from last lecture. Since, $1 - f(0) = \lambda t + o(t)$, we conclude that $\alpha = \lambda$.
%\begin{eqnarray*}
   %&=& f(t) \Pr\{N(s)=0\} \\
   %&=& [1- \lambda s + o(s)] f(t) \\
  %f(t+s) &=& [1- \lambda s + o(s)] f(t)\\
  %\frac{f(t+s)-f(t)}{s} &=& \frac{o(s)}{s}f(t)- \lambda f(t) \\
  %\lim_{s\downarrow0}  \frac{f(t+s)-f(t)}{s}  &=& f(t) \times 0 - \lambda f(t) = -\lambda f(t) \\
 %f'(t) &=& - \lambda f(t), t\geq 0.
%\end{eqnarray*}
%where (a) follows from independent increment property and (b) follows from stationary increment property.We have shown that $f$ has a derivative and is equal to $-\lambda f(t)$.\\
%\begin{eqnarray*}
%% \nonumber to remove numbering (before each equation)
   %\Pr\{N(t)=0\} &=& 1- \lambda (t)+ 0(t) \\
  %\lim _{t\downarrow0}\Pr\{N(t)=0\} &=& 1 \\
  %1= \lim _{t\downarrow0}\Pr\{N(t)=0\}&=& \lim _{t\downarrow0}\Pr\{X_{1}>t\}\\
   %&=& \Pr\{X_{1}>0]  \\
  %\therefore f(0) &=& 1
  %\end{eqnarray*}
  %Solve $\frac{df(t)}{dt} = -\lambda f(t), ~t\geq 0$, $f(0)= 1$. We get $f(t) = e^{-\lambda t}$.
   %\begin{eqnarray*}
  %\therefore f(t) &=&  \Pr\{X_{1}>t\}= e^{-\lambda t}.\\
  %X_{1}&\sim & exp (\lambda ).
%\end{eqnarray*}

%We next show that first two inter-arrival times $X_{1}, X_{2}$ are independent. To this end, note that 
%\begin{equation*}
%\{X_1 = t_1, X_2 = t_2\} \iff \{N(t_1)=1\}\cap\{N(t_2+t_1)-N(t_1)=1\}.
%\end{equation*}
%From stationary independent increment property of $N(t)$, we get  
%\begin{eqnarray*}
 %P(X_{1}=t_{1}, X_{2}=t_{2}) && t_{1} >0, t_{2}>0\\
  %P(X_{1}=t_{1})P (X_{2}=t_{2}| X_{1}=t_{1})&=& \lambda e^{-\lambda t_{1}} \\
   %&=& \lambda e^{-\lambda t_{1}} \Pr\{N_{t_{1}+t_{2}}-\delta=1,N_{t_{1}+t_{2}}=2|N(t_{1})=1) \\
   %&=& \lambda e^{-\lambda t_{1}} \Pr\{Nt_{2}=1] \\
   %&=& \lambda e^{-\lambda t_{1}} \lambda e^{-\lambda t_{2}}  \\
   %X_{1} \perp X_{2}&and & X_{1}, X_{2} \sim exp (\lambda).
%\end{eqnarray*}
%Proof follows from induction
\end{proof}

We study these characterizations because Poisson process is a fundamental process, just like Gaussian process among the class of distributions.