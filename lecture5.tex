\documentclass[a4paper,10pt]{article}
\include{header}
\title{Lecture 5: Limit theorems in Renewal Theory}
\author{Parimal Parag}

\begin{document}
\maketitle
\section{Limit Theorems}

Let $N(\infty) := \lim_{t \to \infty} N(t)$. Then, it is easy to see that $\Pr\{N(\infty) = \infty\} = 1$.
\begin{proof}
It suffices to show $\Pr\{N(\infty) < \infty\} = 0$. We have
\begin{flalign*}
\Pr\{N(\infty) < \infty\} &= \Pr\{\bigcup_{n \in \mathbb{N}} \{N(\infty) < n\}\}\\
&= \Pr\{\bigcup_{n \in \mathbb{N}} \{S_n = \infty\}\} = \Pr\{\bigcup_{n \in \mathbb{N}} \{X_n = \infty\}\} \\
&\leq \sum_{n \in \mathbb{N}}\Pr\{X_n = \infty\} =0.
\end{flalign*}
The last step follows from the fact that $E[X_n] < \infty$.
\end{proof}
\subsection{Basic Renewal Theorem}
We see that $N(t)$ increases to infinity with time. We are interested in rate of increase of $N(t)$ with $t$. Note that $S_{N(t)}$ represents the time of last renewal before $t$, and $S_{N(t)+1}$ represents the time of first renewal after time $t$. 
\begin{prop}
\begin{equation*}
\lim_{t \to \infty} \frac{N(t)}{t} = \frac{1}{\mu} \quad \mbox{almost surely}.
\end{equation*}
\end{prop}
\begin{proof}
% Need picture here
%0 ------- S_{N(t)}---t ---- S_{N(t)+1} ------------

Consider $S_{N(t)}$. By definition, we have
\[S_{N(t)} \leq t < S_{N(t)+1}\]
Dividing by $N(t)$, we get 
\[\frac{S_{N(t)}}{N(t)} \leq \frac{t}{N(t)} < \frac{S_{N(t)+1}}{N(t)}\]
By Strong Law of Large Numbers (SLLN) and the previous result, we have
\[\lim_{t \to \infty}\frac{S_{N(t)}}{N(t)} = \mu \quad \mbox{a.s.}\] 
Also
\[\lim_{t \to \infty} \frac{S_{N(t)+1}}{N(t)} = \lim_{t \to \infty} \frac{S_{N(t)+1}}{N(t)+1}.\frac{N(t)+1}{N(t)} \]
Hence by Squeeze Theorem, the result follows.
\end{proof}
\subsubsection{Example}
Suppose, you are in a casino with infinitely many games. Every game has a probability of win $X$, \emph{iid} uniformly distributed between $(0,1)$. One can continue to play a game or switch to another one. We are interested in a strategy that maximizes the long-run proportion of wins.

Let $N(n)$ denote the number of losses in $n$ plays. Then fraction of wins $P_W(n)$ is given by 
\begin{equation*}
P_W(n) = \frac{n-N(n)}{n}.
\end{equation*}
We pick a strategy where any game is selected to play, and continue to be played till the first loss. Note that, time till first loss is geometrically distributed with mean $\frac{1}{1-X}$. We shall show that this fraction approaches unity as $n \to \infty$. By the previous proposition, we have:
\begin{align*}
\lim_{n \to \infty} \frac{N(n)}{n} &= \frac{1}{E[\mbox{Time till first loss}]} \\
&= \frac{1}{E\left[\frac{1}{1-X}\right]} = \frac{1}{\infty} = 0
\end{align*}
Hence Renewal theorems can be used to compute these long term averages. We'll have many such theorems in the following sections.

\subsection{Wald's Lemma}
Before we get into Wald's Lemma, let us first define what a stopping time is.
\begin{defn}[Stopping Time]
Let $\{X_n: n\in \mathbb{N}\}$ be independent random variables. Then $T$, an integer random variable, is called a stopping time wrt this sequence if $\{N=n\}$ depends only on $\{X_1,\cdots,X_n\}$ and is independent of $X_{n+1}, X_{n+2},\cdots$. 
\end{defn}
Intuitively, if we observe the $X_n$'s in sequential order and $N$ denotes the number observed before stopping then. Then, we have stopped after observing, $X_1, \ldots, X_N$, and before observing $X_{N+1}, X_{N+2}, \ldots$. 
The intuition behind a stopping time is that it's value is determined by past and present events but NOT by future events. 
\begin{exmp}
For instance, while traveling on the bus, the random variable measuring ``Time until bus crosses Majestic and after that one stop" is a stopping time as it's value is determined by events before it happens. On the other hand ``Time until bus stops before Majestic is reached'' would not be a stopping time  in the same context. This is because we have to cross this time, reach Majestic and then realise we have crossed that point. 
\end{exmp}
\begin{exmp} Consider $X_n$ \emph{iid} Bernoulli$(1/2)$. Then $N = min \{n \in \mathbb{N}:\quad \sum_{i=1}^n X_i = 10\}$ is a stopping time.
\end{exmp}
\begin{exmp}[Random Walk Stopping Time] Consider $X_n$ \emph{iid} bivariate random variables with 
\begin{equation*}
\Pr\{X_n = 1\} = \Pr\{X_n = -1\} = \frac{1}{2}. 
\end{equation*}
Then $N = min \{n \in \mathbb{N}:\quad \sum_{i=1}^n X_i = 1\}$ is a stopping time.
\end{exmp}

\textbf{Exercise}: Try to list out properties of stopping times. For instance, sum of two stopping times is a stopping time. Minimum of two stopping times is a stopping time. See how many can you find and prove/disprove.

\begin{lem}[Wald's Lemma]
Let $\{X_i:\quad i\in \mathbb{N}\}$ be \emph{iid} random variables with finite mean $E[X_1]$ and let $N$ be a stopping time with respect to this set of variables, such that $E[N] < \infty$. Then
\[E\left[\sum_{n=1}^N X_n\right] = E[X_1]E[N]\]
\end{lem}
\begin{proof}
\begin{align}
E\left[\sum_{n=1}^N X_n\right] &= E\left[\sum_{n \in \mathbb{N}} X_n 1_{\{N \geq n\}}\right]  \\
&= \sum_{n \in \mathbb{N}} E\left[X_n 1_{\{N \geq n\}}\right] \label{tricky}
\end{align}
I'd like to point out here that in step (\ref{tricky}), you cannot always exchange infinite sums and expectations. But here you can do so. Refer Ross/Wolff if you are interested. Regardless, to proceed, we need to show that $N \geq n$ is independent of $X_k, \quad k \geq n$. To this end, observe that 
\begin{equation*}
\{N \geq k\} = \{N < k\}^c = \{N \leq k-1\}^c = \left(\bigcup_{i=1}^{k-1} \{N = i\}\right)^c. 
\end{equation*}
Since, $N$ is a stopping time and by definition $\{N=i\}$ depends only on $\{X_1,\ldots, X_i\}$. Therefore, $\{N \geq k\}$ depends only on $\{X_1,\ldots, X_{k-1}\}$, and is independent of the future and present samples. Therefore, we can write
\begin{align*}
\sum_{n \in \mathbb{N}} E\left[X_n 1_{\{N \geq n\}}\right] &= \sum_{n \in \mathbb{N}} E\left[X_n\right]E\left[ 1_{\{N \geq n\}}\right] \\
&= E\left[X_1\right] \sum_{n \in \mathbb{N}} \Pr\{N \geq n\} = E[X_1]E[N].
\end{align*} 
\end{proof}

\begin{prop}[Wald's Lemma for Renewal Process] \label{prop:WaldRenewal}
Let $\{X_n, n \in \mathbb{N}\}$ be \emph{iid} inter-arrival times of a renewal process $N(t)$ with $E[X_1] < \infty$, and let $m(t) = E[N(t)]$. Then, $N(t)+1$ is a stopping time and 
 \begin{equation*}
E\left[\sum_{i=1}^{N(t)+1}X_i\right] = E[X_1][1+m(t)]
\end{equation*}
\end{prop}
\begin{proof} It is easy to see that $\{N(t)+1=n\}$ depends solely on $\{X_1,\ldots,X_n\}$ form the equation below.
\begin{equation*}
\left\{N(t) + 1 = n \right\} \iff \{S_{n-1} \leq t < S_n\} \iff \left\{\sum_{i=1}^{n-1} X_i \leq t < \sum_{i=1}^{n-1} X_i + X_n\right\}
\end{equation*}
Thus $N(t)+1$ is a stopping time, and the result follows from Wald's Lemma.
\end{proof}

\subsection{Elementary Renewal Theorem}
Basic renewal theorem implies $N(t)/t$ converges to $1/\mu$ almost surely. Now, we are interested in convergence of $E[N(t)]/t$. Note that this is not obvious, since almost sure convergence doesn't imply convergence in mean. Consider the following example.
\begin{exmp}
\begin{equation*}
Y_n = \begin{cases}
n, & \mbox{ w.p.  } 1/n,\\
0, & \mbox{ w.p.  } 1- 1/n.
\end{cases}
\end{equation*}
Then, $\Pr\{ Y_n = 0 \} = 1 - 1/n$. %Then $\sum \Pr\{Y_n > \epsilon\} < \infty$. Hence by Borel Cantelli Lemma, $Y_n \to 0$
That is $Y_n \to 0$ a.s. However, $E[Y_n] = 1$ for all $n \in \mathbb{N}$. So $E[Y_n] \to 1$.
\end{exmp}
Even though, basic renewal theorem does \textbf{NOT} imply it, we still have $E[N(t)]/t$ converging to $1/\mu$.

\begin{thm}[Elementary Renewal Theorem] Let $m(t)$ denote mean $E[N(t)]$ of renewal process $N(t)$, then under the hypotheses of basic renewal theorem, we have 
\begin{equation*}
\lim_{t \to \infty}\frac{m(t)}{t} = \frac{1}{\mu}.
\end{equation*}
\end{thm}
\begin{proof}
Take $\mu < \infty$. We know that $S_{N(t)+1} > t$. Therefore, taking expectations on both sides and using Proposition~\ref{prop:WaldRenewal}, we have 
\begin{equation*}
\mu (m(t) + 1) > t.
\end{equation*}
Dividing both sides by $\mu t$ and taking $\liminf$ on both sides, we get
\begin{equation}
\label{eq:LiminfMean}
	\liminf_{t \to \infty} \frac{m(t)}{t} \geq \frac{1}{\mu}.
\end{equation}

%Thus we now have to show 
%\[\limsup_{t \to \infty} \frac{m(t)}{t} \leq \frac{1}{\mu}\]
We employ a truncated random variable argument to show the reverse inequality. We define truncated inter-arrival times $\{\bar{X}_n\}$ as 
\begin{equation*}
\bar{X}_n = X_n 1_{\{X_n \leq M\}} + M1_{\{X_n > M\}}.
\end{equation*}
We will call $E[\bar{X}_n] = \mu_M$. Further, we can define arrival instants $\{\bar{S}_n\}$ and renewal process $\bar{N}(t)$ for this set of truncated inter-arrival times $\{\bar{X}_n\}$ as 
\begin{align*}
 \bar{S}_n &= \sum_{k=1}^n \bar{X}_k, & \bar{N}(t) &= \sup\{n \in \mathbb{N}_0: \bar{S}_n \leq t\}.
\end{align*}
Note that since $S_n \geq \bar{S}_n$, number of arrivals would be higher for renewal process with truncated random variables, i.e. 
\begin{equation}
\label{eq:TruncRenewalInequality}
  N(t) \leq \bar{N}(t).
\end{equation}
Further, due to truncation of inter-arrival time, next renewal happens with-in $M$ units of time, i.e.
\begin{equation*}
  \bar{S}_{N(t)+1} \leq t+M.
\end{equation*}
Taking expectations on both sides in the above equation, using Proposition~\ref{prop:WaldRenewal}, dividing both sides by $t \mu_M$ and taking $\limsup$ on both sides, we obtain
\begin{equation*}
\limsup_{t \to \infty}\frac{\bar{m(t)}}{t} \leq \frac{1}{\mu_M}.
\end{equation*}
Taking expectations on both sides of~\eqref{eq:TruncRenewalInequality} and letting $M$ go arbitrary large on RHS, we get
\begin{equation}
\label{eq:LimsupMean}
\limsup_{t \to \infty}\frac{m(t)}{t} \leq \frac{1}{\mu}.
\end{equation}
%Now observe that LHS is independent of $M$. Take limits $M \to \infty$, noting that $\mu_M \to \mu$ (Why?) to get
 %\[\limsup_{t \to \infty}\frac{m(t)}{t} \leq \frac{1}{\mu}\]
%Putting it all together,
%\[\lim_{t \to \infty}\frac{m(t)}{t} = \frac{1}{\mu}\]
Result for finite $\mu$ follows from~\eqref{eq:LiminfMean} and~\eqref{eq:LiminfMean}. When $\mu$ grows arbitrary large, results follow from~\eqref{eq:LiminfMean}, where RHS is zero. 
\end{proof}

\subsection{Central Limit for Renewal Processes}
\begin{thm}
Let $X_n$ be \emph{iid} random variables with $\mu = E[X_n] < \infty$ and $\sigma^2 = Var(X_n) < \infty$. Then
\[\frac{N(t)-\frac{t}{\mu}}{\sigma \sqrt{\frac{t}{\mu^3}}} \to^d N(0,1) \]
\end{thm}
\begin{proof}
Take $u = \frac{t}{\mu} + y \sigma \sqrt{\frac{t}{\mu^3}}$. We shall treat $u$ as an integer and proceed, the proof for general $u$ is an exercise. Recall that $\{N(t) < u\} \iff \{S_u > t\}$. By equating probability measures on both sides, we get
\begin{equation*}
\Pr\{N(t) < u\} = \Pr\left\{\frac{S_u - u\mu}{\sigma \sqrt{u}} > \frac{t - u\mu}{\sigma \sqrt{u}}\right\} = \Pr\left\{\frac{S_u - u\mu}{\sigma \sqrt{u}} > -y\left(1 + \frac{y\sigma}{\sqrt{tu}}\right)^2\right\}.
\end{equation*}
By central limit theorem, $\frac{S_u - u\mu}{\sigma \sqrt{u}}$ converges to a normal random variable with zero mean and unit variance as $t$ grows. Also, note that 
\begin{equation*}
\lim_{t \to \infty} -y\left(1 + \frac{y\sigma}{\sqrt{tu}}\right)^2 = -y.
\end{equation*}
These results combine with the symmetry of normal random variable to give us the result.
\end{proof}
\end{document}