% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 03: Poisson Process}
%\author{}

\begin{document}
\maketitle

\section{Bernoulli Process}


\section{Simple point processes}
%\begin{defn} 
A stochastic process $\{N(t), t\geqslant 0\}$ is a \textbf{point process} if
\begin{enumerate}
  \item $N(0) = 0$, and 
  \item for each $\omega \in \Omega$, the map $t\mapsto N(t)$ is non-decreasing, integer valued, and right continuous.% and at points of discontinuity $(N(t)- N(t^{-}))\leqslant 1,~~\forall \omega \in \Omega$. 
\end{enumerate}
%\end{defn} 
%\begin{defn} 
A point process is \textbf{simple} if the jump size is unity.
%\end{defn}

\begin{figure}[hhhh]
\center
	\input{Figures/Poisson}
	\caption{Sample path of a simple point process.}
	\label{Fig:Poisson}
\end{figure}
%\begin{defn} 
The points of discontinuity correspond to the arrival instants of the point process $N(t)$. 
\textbf{Time of $n^{\text{th}}$ arrival} is a random variable denoted $S_n$, such that  
\begin{xalignat*}{3}
&S_0 = 0, &&S_n = \inf\{t \geq 0: N(t) = n\},~n \in \N.
\end{xalignat*}
%\end{defn}
\begin{lem} Simple point process $\{N(t), t \geqslant 0\}$ and arrival process $\{S_n: n \in \N\}$ are inverse processes. That is,
\begin{align*}
\{S_n \leqslant t\} = \{N(t) \geqslant n\}.
\end{align*}
\end{lem}
\begin{proof} Let $\omega \in \{S_n \leqslant t\}$, then $N(S_n) = n$ by definition. 
%from right continuity of the counting process. 
Since $N$ is a non-decreasing process, we have $N(t) \geq N(S_n) = n$. 
%It follows that $\{S_n \leqslant t\} \subseteq \{N(t) \geqslant n\}$.
Conversely, let $\omega \in \{N(t) \geqslant n\}$, then it follows from definition that $S_n \leq t$.
\end{proof}
\begin{cor} The following identity is true.
\begin{align*}
\{S_n \leqslant t, S_{n+1} > t\} = \{N(t) = n\}.
\end{align*}
\end{cor}
\begin{proof}
It is easy to see that 
\begin{align*}
\{S_{n+1} > t \} &= \{S_{n+1} \leqslant t\}^c = \{N(t) \geqslant n+1\}^c = \{N(t) < n+1\}.
\end{align*}
Hence, the result follows by writing 
\begin{align*}
\{N(t) = n\} &= \{N(t) \geqslant n, N(t) < n+1\} = \{S_n \leqslant t, S_{n+1} > t\}.
\end{align*}
\end{proof}
\begin{lem}
Let $F_n(x)$ be the distribution function for $S_n$, then 
\begin{align*}
P_n(t) \triangleq \Pr\{N(t) = n\} = F_{n}(t)-F_{n+1}(t).
\end{align*}
\end{lem}
\begin{proof} It suffices to observe that following is a union of disjoint events,
\begin{align*}
\{S_n \leqslant t \} &= \{S_n \leqslant t, S_{n+1} > t\} \cup \{S_{n} \leqslant t, S_{n+1} \leqslant t\}.
\end{align*}
\end{proof}
%\begin{defn} 
The \textbf{inter arrival time} between $(n-1)^{th}$ and $n^{th}$ arrival is denoted by $X_n$ and written as
\begin{align*}
X_n = S_n - S_{n-1}.
\end{align*}
%\end{defn}
\begin{rem} For a simple point process, we have
\begin{align*}
\Pr\{X_{n} = 0\} &= \Pr\{X_n\leqslant 0\} = 0.
\end{align*}
%This is equivalent to a point process not having an atom at any point on the line. 
Notice that, generalized point processes in higher dimension don't have any inter-arrival time interpretation. 
\end{rem}
%\begin{defn}
For an interval $I = (s,t]$, the number of arrivals in the interval $I$ is defined as $N(I) = N(t) - N(s)$. 
%\end{defn}
%\begin{defn} 
A point process $\{N(t), t\geqslant 0\}$ has \textbf{stationary increments}, 
if for any collection of mutually exclusive intervals $\{I_j: j \in [n]\}$ and any $t \geqslant 0$ 
\begin{align*}
\Pr\{N(I_j) = k_j, j \in [n]\} &= \Pr\{N(t + I_j), j \in [n]\}.
\end{align*}
%, the joint distribution of $(N(t_{n})-N(t_{n-1}),N(t_{n-1})-N(t_{n-2}),...,N(s))$ is identical to the joint distribution of $(N(t_{n}+t)-N(t_{n-1}+t),...,N(s+t)), ~ \forall t \geqslant 0$.
%\end{defn}
%\begin{defn} 
A point process $\{N(t), t\geqslant 0\}$ has \textbf{stationary  and independent increments}, if 
%if it has stationary increments and the increments are independent random variables.
\begin{align*}
\Pr\{N(t+I_j) = k_j, j \in [n]\} &= \prod_{j \in [n]}\Pr\{N(I_j)\}.
\end{align*}
%\end{defn}

\begin{rem}
For any collection of points  $t_0 = 0 < s<t_{2} < \ldots < t_{n}$, 
we can define mutually exclusive intervals $I_j = (t_{j-1}, t_j]$ for $j \in [n]$. 
Then, a stationary independent increment point process is the one that has the joint probability distribution, a product of marginals and each marginal depends solely on the interval length $|I_j| = t_j - t_{j-1}$. 
That is, 
\begin{align*}
\Pr\{N(I_j) = n_j,{j \in [n]}\} &= \prod_{j \in [n]}\Pr\{N(|I_j|) = n_j\}.
\end{align*} 
\end{rem}

\begin{lem} 
An arrival process $\{S_n, n \in \N_0\}$ has stationary and independent increments iff 
the sequence of inter-arrival times $\{X_n: n \in \N\}$ are \emph{iid} random variables.
\end{lem}
\begin{proof} 
We first suppose that $\{X_n: n \in \N\}$ is a sequence of \emph{iid} random variables. 
Then $S_{n+m} - S_m$ has the same distribution as $S_n$ and is independent of $(X_1, \ldots, X_m)$. 
Conversely, we suppose that $\{S_n: n \in \N_0\}$ has stationary, independent increments. 
Then, $\{X_n: n \in \N\}$ is a sequence of \emph{iid} random variables by looking at $X_n = S_n - S_{n-1}$.   
\end{proof}

\begin{lem} If a simple point process $\{N(t), t \geqslant 0\}$ has stationary and independent increments then 
the sequence of inter-arrival times $\{X_n: n \in \N\}$ are \emph{iid} random variables.
\end{lem}
\begin{proof} 
%Suppose first that $(X_1,X_2,\ldots)$ is a sequence of \emph{iid} random variables, 
%and then $S_{n+m} - S_m$ has same distribution as $S_n$ and is independent of $(X_1, \ldots, X_m)$. 
%Then, for the ordered disjoint intervals $\{I_j = (a_j ,b_j]: j \in [n]\}$ such that $b_{j-1} \leq a_j$ for each $j$, and $K_j \triangleq \sum_{i = 1}^{j}k_i$, we have
%\begin{align*}
%\{N(I_j) = k_j\} &= \{N(b_j) - N(a_j) = k_j\} = \{S_{N(a_j)+k_j+1} - S_{N(a_j)} > |I_j|, S_{N(a_j)+k_j} - S_{N(a_j)} \leqslant |I_j|\}
%\end{align*}
%Then, for the ordered disjoint intervals $\{I_j = (t_{j-1} ,t_j]: j \in [n]\}$ and $K_j \triangleq \sum_{i = 1}^{j}k_i$, we have 
%\begin{align*}
%\Pr\cap_{j \in [n]}\{N(I_j) = k_j\} &= \Pr\cap_{j \in [n]}\{ N(t_j) =  K_j\}\\
%&= \Pr\cap_{j \in [n]}\{X_{K_j+1}+ S_{K_j} - S_{K_{j-1}} > |I_j|, S_{K_j} - S_{K_{j-1}} \leqslant |I_j|\}. 
%\end{align*}
First, we notice that 
\begin{align*}
\{X_{n} > y\} = \{ N(S_{n-1}) \leqslant N(S_{n-1} + y) < N(S_{n})\} = \{N(S_{n-1}+y)-N(S_{n-1}) = 0\}. 
\end{align*}
To show that each inter-arrival time is identically distributed, we utilize the stationary increment property of the counting process $N(t)$, to observe 
\begin{align*}
%\{S_n - S_{n-1} > x\} &= \{N(S_{n-1}+x) - N(S_{n-1}) = 0\}\\
\Pr\{S_n - S_{n-1} > x\} &%= \Pr\{N(S_{n-1}+x) - N(S_{n-1}) = 0\}
= \int_{0}^{\infty}\Pr\{N(x) = 0\}dF_{n-1}(t) = \Pr\{N(x) = 0\} = \Pr\{X_1 > x\}.
\end{align*}
%If $\{X_n\}$ are \emph{iid}, then $S_n$ are independent. 
%Hence, for disjoint intervals $I_j = (a_j, b_j]$, we have 
%\begin{align*}
%\Pr\cap_{j=1}^{n}\{N(I_j) = n_j\} &= \Pr\cap_{j=1}^{n}\{ S_{n_j} - S_{}\leq a_j, S_{n_j+1} > b_{j}\} = \int_{}.
%\end{align*}
To show that inter-arrival times are independent, it suffices to show that $X_{n}$ is independent of $S_{n-1}$. 
% to show that all inter-arrival times are independent. 
Using the independent increments property, we see that 
%\begin{align*}
%\{S_n \leqslant x, S_{n+1} - S_n > y\} &= \{S_n \leqslant x, N(y+S_n) - N(S_n) = 0\},\\
%&=\{n = N(S_n) \leqslant N(x), N(y+S_n)-N(S_n)\}. 
%\end{align*}
%Further, we observe that
\begin{align*}
\Pr\{S_{n-1} \leqslant x, X_{n} > y\} &%= \Pr\{N(S_{n-1} + y) - N(S_{n-1}) = 0, S_{n-1} \leqslant x\} 
= \int_{0}^{x}\Pr\{N(y+t) - N(t) = 0|S_{n-1} = t\}dF_{n-1}(t)\\
&= \int_{0}^{x}\Pr\{N(y+t) - N(t) = 0|N(t) = n-1, N(s) < n-1, s < t \}dF_{n-1}(t) \\
&= \Pr\{X_n > y\}F_{n-1}(x).
\end{align*}

\end{proof}

\section{Poisson process}
\begin{lem} A unique non-negative right continuous function $f: \R \to \R$ satisfying equation
\begin{align*}
 f(t+s) = f(t)f(s), \text{ for all } t,s \in \R
 \end{align*}
 is $f(t) = e^{\theta t}$, where $\theta = \log f(1)$.
\end{lem}
\begin{proof}[Omit]
Clearly, we have $f(0) = f^2(0)$. Since $f$ is non-negative, it means $f(0) = 1$. By definition of $\theta$ and induction for $m,n \in \Z^+$, we see that 
\begin{xalignat*}{3}
&f(m) = f(1)^m = e^{\theta m},&& e^{\theta} = f(1) = f(1/n)^n.
 \end{xalignat*}
Let $q \in \Q$, then it can be written as $m/n, n \neq 0$ for some $m,n \in \Z^+$. 
Hence, it is clear that for all $q \in \Q^+$, we have $f(q) = e^{\theta q}$.
either unity or zero. Note, that $f$ is a right continuous function and is non-negative. 
%We will show that $g$ is an exponential function. That is, $g(t) = e^{\theta t}$ for some $\theta \geqslant 0$. We will prove this in stages. First, we show this is true for $t \in \mathbb{Z}^+$. Notice that we can obtain via induction
%\begin{eqnarray*}
%	g(2) &=& g(1) g(1) = g^{2}(1), \mathrm{ and }\\
%	g(m) &=& [g(1)]^{m}.
%\end{eqnarray*}
%Since $g(1)$ is non negative, there exists a $\beta$ such that $g(1)=e^{\beta}$ and $g(m)= e^{m \beta}, m \in \mathbb{Z}_{+}$. Next we show that for any $n \in \mathbb{Z}_{+}$,        
%\begin{align*}
%	g(1) =  g\left(\frac{1}{n}+..., +\frac{1}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{n}.
%\end{align*}
%Therefore, for same $\beta$ we used for $g(1)$, we have $g\left(\frac{1}{n}\right) = e^{\frac{\beta}{n}}$. Now, we show that $g$ is exponential for any $t \in \mathbb{Q}^+$. To this end, we see that for any $m, n \in \mathbb{Z}_{+}$, we have 
%\begin{align*}
%	g\left(\frac{m}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{m}= e^{\frac{m \beta}{n}}.
%\end{align*}
Now, we can show that $f$ is exponential for any real positive $t$ by taking a sequence of rational numbers $\{t_n\}$ decreasing to $t$. From right continuity of $g$, we obtain 
\begin{align*}
	g(t) = \lim_{t_n \downarrow t} g(t_n) =   \lim_{t_n \downarrow t} e^{\beta t_{n}}= e^{\beta t}.
\end{align*}
\end{proof}
%\begin{defn} 
A random variable $X$ with continuous support on $\R_+$, is called \textbf{memoryless} if %for all $t,s \in \R_+$, we have
\begin{align*}
  \Pr\{X>s\} &= \Pr\{ X > t+s| X>t\}~\forall t,s \in \R_+.% = \frac{ \Pr\{ X>t+s, X>t\}}{\Pr\{X>t\}}.
\end{align*}
%\end{defn}
\begin{prop} The unique memoryless distribution function with continuous support on $\R_+$ is the exponential distribution.
\end{prop}
\begin{proof}
Let $X$ be a random variable with a distribution function $F: \R_+ \to [0,1]$ with the memoryless property. 
Let $g(t) \triangleq 1 - F(t)$. It follows from the memoryless property of $F$, that
\begin{align*}
 g(t+s) = g(t)g(s).
 %\Pr\{X>s\} &= \Pr\{ X > t+s| X>t\} \\&= \frac{ \Pr\{ X>t+s, X>t\}}{\Pr\{X>t\}}.
\end{align*}
%Since $\{X > t + s\} =\{ X>t+s, X>t\}$, we have $g(t+s) = g(t)g(s)$ and hence $g(0) = g^2(0)$. Therefore, $g(0)$ is either unity or zero. Note, that $g$ is a right continuous function and is non-negative. 
%
%We will show that $g$ is an exponential function. That is, $g(t) = e^{\theta t}$ for some $\theta \geqslant 0$. We will prove this in stages. First, we show this is true for $t \in \mathbb{Z}^+$. Notice that we can obtain via induction
%\begin{eqnarray*}
%	g(2) &=& g(1) g(1) = g^{2}(1), \mathrm{ and }\\
%	g(m) &=& [g(1)]^{m}.
%\end{eqnarray*}
%Since $g(1)$ is non negative, there exists a $\beta$ such that $g(1)=e^{\beta}$ and $g(m)= e^{m \beta}, m \in \mathbb{Z}_{+}$. Next we show that for any $n \in \mathbb{Z}_{+}$,        
%\begin{align*}
%	g(1) =  g\left(\frac{1}{n}+..., +\frac{1}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{n}.
%\end{align*}
%Therefore, for same $\beta$ we used for $g(1)$, we have $g\left(\frac{1}{n}\right) = e^{\frac{\beta}{n}}$. Now, we show that $g$ is exponential for any $t \in \mathbb{Q}^+$. To this end, we see that for any $m, n \in \mathbb{Z}_{+}$, we have 
%\begin{align*}
%	g\left(\frac{m}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{m}= e^{\frac{m \beta}{n}}.
%\end{align*}
%Now, we can show that $g$ is exponential for any real positive $t$ by taking a sequence of rational numbers $\{t_n\}$ decreasing to t. From right continuity of $g$, we obtain 
%\begin{align*}
%	g(t) \stackrel{(a)}{=} \lim_{n\rightarrow \infty} g(t_n) =   \lim_{n\rightarrow \infty} e^{\beta t_{n}}= e^{\beta t}.
%\end{align*}
Since $g(x) = \Pr\{X > x\}$  is non-increasing in $x \in \R_+$, we have $g(x) = e^{\theta x}$, where $\theta < 0$.
\end{proof}

%\begin{defn} 
A simple point process $\{N(t),~ t\geqslant 0\} $ is called a \textbf{Poisson process} with a finite positive rate $\lambda$, 
if the inter-arrival times $\{X_{n}: n \in \N\}$ are \emph{iid} random variables with an exponential distribution of rate $\lambda$. 
That is, it has a distribution function $F$, such that 
 \begin{align*}
 F(x) = \Pr\{X_{1}\leqslant x\} = 
	\begin{cases}
		1-e^{-\lambda x}, & x\geqslant 0   \\
		0,  & \text{ else}.
	\end{cases}
\end{align*}
%\end{defn}

\begin{figure}[hhhh]
\center
	\input{Figures/IndependentIncrements}
  	\caption{Stationary independent increment property of Poisson process.}
	\label{Fig:IndependentIncrements}
\end{figure}
\begin{prop} A Poisson process $\{N(t), t\geqslant 0\}$ is simple point process with stationary independent increments.
\end{prop}
\begin{proof}
It is clear that Poisson process is a simple point process. 
To show that $N(t)$ has stationary independent increment property, it suffices to show that $N_t-N(s) \perp N(s)$ and $N(t) - N(s) \sim N(t-s)$. 
This follows from the fact that we can use induction to show stationary independent increment property for for any finite disjoint time-intervals.

Let arrival time-instants $\{S_n: n \in \N_0\}$ and inter-arrival times $\{X_n: n \in \N\}$ be defined as before. 
Given any time $s$, we can define the following variables
\begin{xalignat*}{3}
&X'_{N(s)+1} = s - S_{N(s)},&&X''_{N(s)+1} = S_{N(s)+1} - s.
\end{xalignat*}
It is clear that $s$ partitions $X_{N(s)+1}$ in two parts such that $X_{N(s)+1} = X^{'}_{N(s)}+1 + X^{''}_{N(s)+1}$ as seen in Figure~\ref{Fig:IndependentIncrements} for the case when $N(s) = n$. 
We look at joint distribution of $X'_{N(s)+1}, X''_{N(s)+1}$ and notice that
\begin{align*}
\{X'_{N(s)+1}  > x, X''_{N(s)+1} > y\} %&= \bigcup_{n \in \N_0}\{X' \leq x, X'' > y, N(s) = n\},\\
&= \bigcup_{n \in \N_0}\{ S_n <  s -x , S_{n+1} > s + y, N(s) = n\}\\
&= \bigcup_{n \in \N_0}\{ S_n <  s -x , S_{n+1} > s + y\}.
\end{align*}
From the fact that inter-arrival times are \emph{iid} exponentially distributed with rate $\lambda$, we conclude that
\begin{align*}
\Pr\{X'_{N(s)} > x, X''_{N(s)+1} > y\} &= \sum_{n \in \N_0}\int_{u=0}^{s-x}\Pr\{X_{n+1} > s + y + u \}dF_n(u),\\
&= \int_{u=0}^{s-x}(1 - F_1(s+y+u))\sum_{n \in \N_0}dF_n(u) = \int_{u=0}^{s-x}e^{-\lambda(s+y+u)}\lambda du,\\
&= (1-F_1(y))(F_1(s) - F_1(2s-x)).
\end{align*}
Therefore,  $X_{N(s)+1}^{''}$ is independent of $X_{N(s)+1}^{'}$ and has same distribution as $X_{n+1}$. 
The memoryless property of exponential distribution is crucially used. 
Further, we see that independent increment holds only if inter-arrival time is exponential. 
Therefore, 
\begin{align*}
\{ N(s) = n \} &\iff  \{ S_n = s + X'_{n+1} \}, \\
\{ N(t) - N(s) \geqslant m \} &\iff \{ X''_{n+1} + \sum_{i=n+2}^{n+m} X_i \leqslant t - s \}.
\end{align*}
Since, $\{X_i: i \geqslant n+2\}\cup\{X_{n+1}^{''}\}$ are independent of $\{X_i: i \leqslant n\}\cup{X_{n+1}^{'}}$, we have $N(t)-N(s) \perp N(s)$. Further, since $X_{n+1}^{''}$ has same distribution as $X_{n+1}$, we get $N(t) - N(s) \sim N(t-s)$. By induction we can extend this result to $(N(t_{n})-N(t_{n-1}),...,N(s))$. 
\end{proof}

\begin{thm}[Characterization 1] A simple stationary independent increment process is a Poisson process with parameter $\lambda$ when
\begin{xalignat*}{3}
&\lim_{t \to 0}\frac{\Pr\{N(t) = 1\}}{t} = \lambda,&&\lim_{t \to 0}\frac{\Pr\{N(t) \geq 2\}}{t} = 0.
\end{xalignat*}
\end{thm}
\begin{proof}
It suffices to show that first inter-arrival times $X_1$ is exponentially distributed with parameter $\lambda$. Notice that
\begin{align*}
P_0(t+s) = \Pr\{N(t+s) - N(t) = 0, N(t) = 0\} = P_0(t)P_0(s).
\end{align*}
Using the conditions in the theorem, the result follows.
\end{proof}
\subsection{Distribution functions}
\begin{lem} Moment generating function of arrival times $S_n$ is 
 \begin{align*}
  \mathbb{E} [ e^{\theta S_n} ] = 
		\begin{cases}
		\frac{\lambda^n}{(\lambda-\theta)^n}, & \theta < \lambda \\
		\infty, & \theta \geqslant \lambda.
		\end{cases} 
 \end{align*} 
 Distribution function of $S_n$ is given by 
 \begin{align*}.
 \end{align*}
\end{lem}
\begin{proof} 
Since $S_n = \sum_{k=1}^nX_k$, where $X_k$ are \emph{iid}, the moment generating function $\mathbb{E} [ e^{\theta S_{n}} ]$ of $S_n$ is 
 \begin{align*}
  \mathbb{E} [ e^{\theta S_{n}} ] = \left(\mathbb{E}[e^{\theta X_{1}}]\right)^{n}. 
 \end{align*} 
Since each $X_k$ is \emph{iid} exponential with rate $\lambda$, it is easy to see that moment generating function of inter-arrival time $X_1$ is 
 \begin{align*}
  \mathbb{E} [ e^{\theta X_1} ] = 
		\begin{cases}
		\frac{\lambda^n}{(\lambda-\theta)^n}, & \theta < \lambda \\
		\infty, & \theta \geqslant \lambda.
		\end{cases} 
 \end{align*} 
\end{proof}

\begin{thm} Density function of $S_n$ is Gamma distributed with parameters $n$ and $\lambda$. That is,
\begin{align*}
f_{n}(s) =\frac{\lambda (\lambda s)^{n-1}} {(n-1)!} e^{-\lambda s}.
\end{align*}
\end{thm}
\begin{proof} Notice that $X_i$'s are \emph{iid} and $S_1 = X_1$. In addition, we know that $S_n = X_n + S_{n-1}$. Since, $X_n$ is independent of $S_{n-1}$, we know that distribution of $S_n$ would be convolution of distribution of $S_{n-1}$ and $X_1$. Since $X_n$ and $S_1$ have identical distribution, we have $f_{n}=f_{n-1}*f_{1}$. The result follows from straightforward induction.
\end{proof}

%Process $N(t)$ is of real interest, and we can compute the distribution of $N(t)$ for each $t$ from the distribution of $S_n$ in the following.
%\begin{figure}[hhhh]
%\center
	%\include{./Figures/Poisson}
 %% \caption{}\label{}
%\end{figure}
\begin{thm} For each $t >0$, the distribution of Poisson process $N(t)$ with parameter $\lambda$ is given by
	\begin{align*}
	\Pr\{N(t)=n)\}= e^{-\lambda t}\frac{(\lambda t)^{n}}{n!}.
	\end{align*}
Further, $E[N(t)] = \lambda t$, explaining the rate parameter $\lambda$ for Poisson process.
\end{thm}
\begin{proof}
Result follows from density of $S_n$ and recognizing that 
\begin{align*}
P_n(t) = F_n(t) - F_{n+1}(t).
\end{align*}
%\begin{eqnarray*}
%   \Pr\{N(t) =n\}&=&  \Pr\{S_{n}\leqslant t, S_{n+1} >t)\\
%   &=&  \int^{t}_{0} \Pr\left\{ {S_{n+1}>t}|{S_{n}=s}\right\}f_{n}(s)  ds\\
%   &\stackrel{(a)}{=}& \int^{t}_{0} \Pr\{X_{n+1}>t-s\} f_{n}(s) ds\\
%   &=&  \int^{t}_{0}e^{-\lambda(t- s)} \frac{\lambda^{n}s^{n-1}}{(n-1)!}e^{-\lambda s}  ds\\
%   &=&\frac{e^{-\lambda t} (\lambda t)^{n}}{n !}.
%\end{eqnarray*}
% where (a) follows from the memoryless property of exponential distribution. %($\Pr\{X_{n+1}>s+t|X_{k+1}>t)=\Pr\{X_{k+1}>s)$).\\
\end{proof}

\begin{cor} Distribution of arrival times $S_n$ is 
\begin{xalignat*}{3}
&F_n(t)= \sum_{j \geq n}P_j(t),&&\sum_{n \in \N}F_n(t) = \E N(t).%e^{-\lambda t}\frac{(\lambda t)^{j}}{j!}.
\end{xalignat*}
%Further, $\sum_{n \in \N_0}F_n(t) = 1 + \lambda t$.
\end{cor}
\begin{proof}
First result follows from the telescopic sum and the second from the following observation.
\begin{align*}
\sum_{n \in \N}F_n(t) &= \E\sum_{n \in \N}1\{N(t) \geqslant n\} = \sum_{n \in \N}\Pr\{N(t) \geqslant n\} = \E N(t).
\end{align*}
\end{proof}
%\begin{proof}[Omit]
%Result follows from distribution of $P_n(t)$ and recognizing that $F_n(t) =  \sum_{j \geq n}P_j(t)$.
%Further, we notice that
%\begin{align*}
%\sum_{n \in \N_0}F_n(t) &=  \sum_{n \in \N_0}\sum_{j \geq n}P_j(t) = \sum_{i \in \N_0}\sum_{n = 0}^{j}P_j(t) = \sum_{i \in \N_0}(j+1)P_j(t)\\
%&= 1 + E[N(t)] = 1 + \lambda t.
%\end{align*}
%\end{proof}


\begin{rem} A Poisson process is not a stationary process. That is, the finite dimensional distributions are not shift invariant. 
\end{rem}
%In the following section, we show that the Poisson process is a \textit{stationary,  independent increment} process. To this end, we will use an important property of exponential distribution- namely memoryless property. Memoryless property of exponential distribution will facilitate the computation of fdd of the Poisson process via one dimensional marginal distribution. 
\begin{lem} For any finite time $t > 0$, a Poisson process is finite almost surely.
\end{lem}
\begin{proof} By strong law of large numbers, we have 
\begin{align*}
\lim_{n \to \infty} \frac{S_{n}}{n} = E[X_{1}] = \frac{1}{\lambda}\quad\mathrm{a.s.} 
\end{align*}
%Therefore, we have $S_n \rightarrow \infty$, a.s. This implies $\Pr\{N(t) < \infty\} =1$. To see this, 
Fix $t > 0$ and let $M = \{\omega \in \Omega: N(t)(\omega) = \infty \}$ be a subset of the sample space. Let $\omega \in M$, then $S_{n}(\omega)\leqslant t$ for all $n \in \N$. This implies $\lim\sup_n\frac{S_{n}}{n} = 0$  and $\omega \not\in \{\lim_n \frac{S_{n}}{n} = \frac{1}{\lambda} \}.$ Hence, the probability measure for set $M$ is zero. 
\end{proof}

\begin{prop}[Characterization 2] Let $\{I_i \subseteq \R_+: 1 \leq i \leq k\}$ be a sequence of disjoint intervals. A stationary independent increment point process $\{N(t),~t\geqslant 0\}$, such that $N(0) = 0$ is Poisson process iff 
%\begin{figure}[h!]
%\center
  %% Requires \usepackage{graphicx}
  %\includegraphics[width=2.8in, height=0.9in]{./Figures/SPQT.png}\\
 %% \caption{}\label{}
%\end{figure}
\begin{align*}
  \Pr\{\bigcap_{i=1}^k \{N(I_i)= n_{i}\}\} = \prod_{i=1}^{k}\frac{(\lambda|I_i|)^{n_{i}}}{n_{i}!} e^{-\lambda |I_i|}.
\end{align*}
\end{prop}
\end{document}