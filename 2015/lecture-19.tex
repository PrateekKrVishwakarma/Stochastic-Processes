\documentclass[a4paper,10pt]{article}
\usepackage{%
amsmath,%
amsfonts,%
amssymb,%
amsthm,%
hyperref,%
url,%
latexsym,%
epsfig,%
graphicx,%
psfrag,%
subfigure,%
color,%
tikz,%
pgf,%
pgfplots,%
pgfplotstable,%
pgfpages%
}
\usepgflibrary{shapes}
\usetikzlibrary{%
arrows,%
backgrounds,%
chains,%
decorations.pathmorphing,% /pgf/decoration/random steps | erste Graphik
decorations.text,%
matrix,%
positioning,% wg. " of "
fit,%
patterns,%
petri,%
plotmarks,%
scopes,%
shadows,%
shapes.misc,% wg. rounded rectangle
shapes.arrows,%
shapes.callouts,%
shapes%
}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{exmp}[thm]{Example}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{note}[thm]{Note}
\date{}
\title{Lecture 19: Random Walks}
\author{Parimal Parag}
\pgfplotsset{compat=1.12}
\begin{document}
\maketitle
\section{Introduction}
Let $\{X_i\}$ be iid random variables with $E[|X_1|] < \infty$. Let $S_0=0$. Then the process
\[S_n = \sum_{k=1}^n X_i \quad n \geq 1\]
is called a \textbf{random walk (RW)} process. A random walk is called a Simple Random walk or (SRW) if 
\[P[X_1 = 1] = p = 1- P[X_1 = -1]\]
This has the interpretation of the winnings of a Gambler who plays a simple coin toss game and wins Re.1 if heads and loses Re. 1 if tails. Random walks are useful in analyzing GI/GI/1 Queues, Ruin systems and even stock prices !!

\section{Duality in Random Walks}
Essentially, if $X_i$ are iid, then $X_1,X_2,\cdots,X_n$ has the same joint distribution as $X_n, X_n-1,\cdots, X_1$. The first result we shall show that if $E[X_1] >0 $ then the random walk will become positive in finite steps. 
\begin{prop}
Suppose $X_1,X_2,\cdots$ are iid random variables, $S_n = \sum_k=1^n X_i$ with $E[X_i] >0$. If 
\[N = \min \{n > 0: S_n > 0\}\]
Then $E[N] < \infty$
\end{prop}
\begin{proof}
\begin{flalign}
E[N] &= \sum_{n=0}^\infty P[N > n] \\
&= \sum_{n=0}^\infty P[X_1 \leq 0, X_1+X_2 \leq 0, \cdots, X_1+X_2+\cdots+X_n \leq 0]  \label{Dual1}\\
&= \sum_{n=0}^\infty P[X_n \leq 0, X_n+X_{n-1} \leq 0, \cdots, X_n+X_{n-1}+\cdots+X_1 \leq 0] \label{Dual2}\\
&= \sum_{n=0}^\infty P[S_n \leq S_{n-1},S_{n} \leq S_{n-2}, \cdots, S_n \leq 0]
\end{flalign}
Where we used the duality principle to get (\ref{Dual2}). Now let us count a renewal at time $n$ when $S_n \leq S_{n-1},S_{n} \leq S_{n-2}, \cdots, S_n \leq 0$. Then we get
\begin{flalign}
E[N] &= \sum_{n=0}^\infty P[\mbox{renewal happens at time n}]\\
&= 1 + E[\mbox{No of renewals that occur}]
\end{flalign}
As $S_n \to \infty$ by Strong Law of Large numbers, it follows that the expected number of renewals that occur is finite. Thus $E[N] < \infty$.
\end{proof}
Define the range $R_n$ as the number of distinct values of $(S_0,\cdots, S_n)$.

\begin{prop}
\[ \lim_{n \to \infty} \frac{E[R_n]}{n} = P[\mbox{ Random walk never returns to zero}] \]
\end{prop}
\begin{proof}
Define
\[I_k = \left \{ \begin{array}{cr}
1 & \mbox{if } S_k\neq S_{k-1}, S_k\neq S_{k-2}, \cdots, S_k \neq S_0 \\
0 & \mbox{else}
\end{array} \right.\]
Then
\[R_n = 1 + \sum_{k=1}^nI_k\]
Now by expanding and using the Duality principle, we get
\begin{flalign}
E[R_n] &= 1 + \sum_{k=1}^n P[X_1 \neq 0, X_1+X_2 \neq 0, \cdots, X_1+X_2+\cdots + X_k \neq 0] \\
&= 1 + \sum_{k=1}^n P[S_1 \neq 0, S_2 \neq 0, \cdots, S_k \neq 0] \\
&= \sum_{k=0}^n P[T > k]
\end{flalign}
Where $T$ is the first time the RW returns to 0. Since 
\[\lim P[T > k] = P[\mbox{RW never returns to 0}]\], we get our result by dividing by $n$ and taking limits.
\end{proof}

\begin{exmp}
\textbf{Simple Random Walk}. Consider the simple random walk with $P[X_i = 1] = p$. When $p=\frac{1}{2}$, the RW is recurrent and thus
\[P[\mbox{No Return to 0}] = 0\]
When $p > \frac{1}{2}$, let $\alpha = P[\mbox{return to 0}|X_1 = 1]$. Since $P[\mbox{return to 0}|X_1 = -1] = 1$ by Law of Large numbers, we have
\[P[\mbox{Return to 0}] = \alpha p+ 1-p\]
Conditioning on $X_2$ yields 
\[\alpha = \alpha^2 p + 1-p\]
Solving for $\alpha$ yields
\[\alpha = \frac{1-p}{p}\]
Thus for $p > 1/2$,
\[\frac{E[R_n]}{n} \to 2p-1\]
And for $p \leq 1/2$
\[\frac{E[R_n]}{n} \to 2(1-p)-1\]
\end{exmp}

\begin{prop}
In the symmetric RW $(p=1/2)$, the expected number of visits to state $k$ before returning to origin is equal to $1$ for all $k \neq 0$.
\end{prop}

\begin{proof}
For $k>0$, let $Y$ denote the number of visits to state k before the first return to origin. Then
\[Y = \sum_{n=1}^\infty I_n\]
where
\[I_n = \left\{ \begin{array}{cl}
	1& \mbox{ if a visit is made to state k at time n and}\\
	& \mbox{there is no return to origin before n}\\
	0 & \mbox{ else}
\end{array}\right.\] 
Thus
\begin{flalign}
E[Y] &= \sum_{n=1}^\infty P[S_n > 0, S_{n-1}> 0, \cdots, S_1>0, S_n = k]\\
&= \sum_{n=1}^\infty P[X_1+X_2 +\cdots X_n > 0, X_2+X_3+\cdots +X_n> 0, \cdots, X_n>0, S_n = k]\\
&= \sum_{n=1}^\infty P[S_n > 0, S_n > S_1, \cdots, S_n > S_{n-1}, S_n=k] \\
&= \sum_{n=1}^\infty P[\mbox{Symmetric RW hits k for first time at time n}] \\
&= P[\mbox{Symmetric RW ever hits k}] = 1 \mbox{ due to recurrence}
\end{flalign}
\end{proof}
\end{document}